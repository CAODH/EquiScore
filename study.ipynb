{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from torch_geometric.nn import NNConv, Set2Set,GATConv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "import torch \n",
                "load_save_file = '/home/caoduanhua/score_function/GNN/train_result/graphnorm/graphformer/GAT_gate/2021-12-24-05-33-55/save_best_model.pt'\n",
                "state_dict = torch.load(load_save_file,map_location = 'cpu')\n",
                "model_dict = state_dict['model']\n",
                "# model_dict.pop('deta')\n",
                "# torch.zeros_like(torch.ones(10))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "OrderedDict([('mu', tensor([2.7181])),\n",
                            "             ('dev', tensor([4.0602])),\n",
                            "             ('deta', tensor([0.5000])),\n",
                            "             ('atom_encoder.weight',\n",
                            "              tensor([[-1.1968e-01, -7.1418e-02,  6.2061e-02,  ..., -1.3653e-01,\n",
                            "                       -1.9085e-02, -1.3483e-01],\n",
                            "                      [ 2.2582e-01, -2.8367e-01,  3.3152e-01,  ..., -1.5755e-01,\n",
                            "                        9.6486e-03,  8.9180e-02],\n",
                            "                      [ 1.2511e-01,  1.8614e-01, -1.8672e-02,  ...,  3.0275e-01,\n",
                            "                        3.1771e-01,  4.2742e-01],\n",
                            "                      ...,\n",
                            "                      [-3.9851e-02,  2.9977e-01,  9.4150e-03,  ...,  3.7962e-02,\n",
                            "                        2.8746e-01, -6.2698e-02],\n",
                            "                      [-1.6643e-01, -2.0618e-02, -1.1374e-01,  ..., -1.5065e-02,\n",
                            "                       -7.0697e-02, -1.2009e-01],\n",
                            "                      [-3.0235e-02,  8.4255e-02, -1.7645e-01,  ..., -3.0323e-05,\n",
                            "                       -7.8984e-02,  9.9916e-02]])),\n",
                            "             ('in_degree_encoder.weight',\n",
                            "              tensor([[ 1.7398e-01,  1.3017e-01, -2.0520e-02,  1.1666e-01, -2.3884e-01,\n",
                            "                       -7.2396e-02,  5.0269e-02, -2.5161e-01, -4.9511e-02, -2.8310e-02,\n",
                            "                        1.5673e-01,  6.4550e-02,  1.4970e-01, -1.3125e-01, -2.3545e-02,\n",
                            "                        3.5764e-02,  1.4739e-02, -1.5516e-01,  1.6419e-01, -2.1990e-01,\n",
                            "                       -6.5923e-02, -2.6678e-01,  8.0967e-02, -5.7116e-02, -2.0934e-01,\n",
                            "                        2.1583e-01, -4.1427e-02, -1.1008e-01,  7.6735e-02, -3.7191e-02,\n",
                            "                        1.0192e-01,  6.1102e-02,  9.7107e-02, -1.2876e-01, -1.6788e-02,\n",
                            "                        1.9501e-02, -6.2135e-02,  3.8968e-02, -4.5586e-02, -2.3678e-02,\n",
                            "                       -1.7869e-01,  2.2595e-01, -1.9481e-01,  1.4963e-01,  4.5030e-02,\n",
                            "                        7.2592e-02,  9.0687e-02, -1.3465e-01,  1.4088e-01, -1.8212e-01,\n",
                            "                        8.0354e-02, -2.7510e-03, -1.9664e-02,  2.8192e-01, -8.7403e-03,\n",
                            "                        2.4797e-02,  2.6601e-01, -1.8278e-02, -1.4642e-01,  6.2538e-02,\n",
                            "                        8.2199e-02, -2.6561e-02, -5.6223e-02, -1.6828e-02,  1.0241e-01,\n",
                            "                        4.0692e-02,  3.0421e-01, -1.1142e-01, -2.4928e-01, -1.2494e-01,\n",
                            "                        8.9613e-02,  2.0508e-01,  9.2829e-03,  1.1206e-01,  2.9419e-02,\n",
                            "                        1.2726e-01,  1.4123e-01, -3.7818e-01, -1.9763e-01,  3.3691e-01],\n",
                            "                      [ 5.1665e-02, -9.0335e-02,  7.0821e-02,  1.1928e-01, -3.0473e-01,\n",
                            "                        1.7411e-01,  7.7678e-03, -1.2809e-01, -8.1048e-02, -1.1769e-01,\n",
                            "                        1.6916e-01,  2.2886e-02, -4.4723e-03, -1.4669e-01, -8.2507e-03,\n",
                            "                        2.9445e-01,  1.7698e-01, -9.1500e-02,  1.4479e-01, -3.1582e-01,\n",
                            "                       -7.3061e-02, -2.5968e-01, -4.4099e-02, -6.2919e-01, -1.3821e-01,\n",
                            "                       -2.3330e-01, -2.7731e-01,  8.4923e-02,  1.1755e-01, -3.3525e-02,\n",
                            "                       -1.6111e-02, -5.2080e-02, -2.9139e-01, -1.9858e-01, -1.2214e-01,\n",
                            "                       -2.9499e-01,  1.0699e-01, -8.1935e-02, -1.0008e-02,  3.8554e-03,\n",
                            "                        7.9540e-02,  1.9931e-01, -2.1701e-01,  8.8333e-02,  8.8558e-02,\n",
                            "                       -4.9215e-01,  3.3681e-02,  1.8090e-01, -4.4668e-02,  8.5715e-02,\n",
                            "                       -1.4578e-01, -1.2021e-02,  8.0743e-03,  6.9164e-02,  7.8722e-02,\n",
                            "                       -7.1174e-02,  9.4823e-02, -1.0858e-01, -6.5335e-02,  1.3534e-01,\n",
                            "                       -3.9206e-02, -2.2806e-01, -6.3481e-02,  2.9019e-02, -2.8245e-02,\n",
                            "                        1.4941e-01, -1.0687e-02, -5.9527e-02, -5.1234e-03, -3.2833e-02,\n",
                            "                       -9.7009e-02, -2.6679e-01,  6.1905e-02,  1.3679e-01, -3.0217e-01,\n",
                            "                        2.0012e-01, -9.9701e-02,  4.7498e-02, -1.3142e-01, -3.6277e-02],\n",
                            "                      [ 6.8147e-02, -2.4843e-01, -2.0338e-02, -2.1598e-02, -1.1461e-01,\n",
                            "                       -7.4547e-02, -5.6197e-02, -7.5890e-02, -1.5894e-02,  3.9469e-03,\n",
                            "                        3.5271e-02, -3.1325e-01, -2.5313e-02, -7.3264e-02,  1.5820e-02,\n",
                            "                        8.6750e-02, -1.7672e-01, -3.9643e-02,  7.1052e-02,  2.4112e-01,\n",
                            "                       -1.0763e-01, -7.0585e-02,  5.4196e-03,  6.5724e-02,  1.4835e-02,\n",
                            "                        2.3212e-01,  6.9541e-02, -4.1226e-02, -6.4287e-02,  1.8758e-01,\n",
                            "                       -1.1878e-01,  1.2898e-01,  5.5065e-02, -2.0833e-01,  2.3880e-01,\n",
                            "                       -1.2464e-02, -8.2663e-02,  2.5817e-02,  1.1455e-01,  1.7318e-01,\n",
                            "                        5.2119e-02, -1.7360e-03,  1.1322e-01, -1.2537e-01,  3.7224e-02,\n",
                            "                       -1.0887e-01, -1.8003e-01, -2.0466e-01, -1.2970e-01,  5.0385e-02,\n",
                            "                        8.6977e-02,  2.0902e-01, -2.2278e-01,  2.3981e-01,  3.0472e-02,\n",
                            "                        8.2690e-02, -1.9039e-01, -2.2635e-02,  9.8777e-02, -3.7379e-02,\n",
                            "                       -9.7479e-02,  1.8008e-01, -2.4656e-01,  6.8853e-02, -8.4490e-02,\n",
                            "                        7.3028e-02, -2.4948e-03,  2.1866e-01,  1.8206e-01,  1.6417e-01,\n",
                            "                       -1.0736e-01,  9.1809e-02,  2.1660e-01,  1.8127e-01,  1.1432e-01,\n",
                            "                        1.3506e-01,  3.0035e-01, -2.0779e-01, -3.9410e-01,  1.8553e-01],\n",
                            "                      [-1.2796e-01,  1.6174e-02,  1.7772e-02, -4.8092e-02,  1.7937e-01,\n",
                            "                       -8.9969e-02,  3.4960e-02, -1.9728e-01, -1.7258e-01,  1.4149e-01,\n",
                            "                       -9.9047e-02, -7.0629e-02, -4.8635e-02, -6.1973e-02, -4.3675e-02,\n",
                            "                       -1.4323e-02,  1.8638e-01, -1.1171e-01, -1.6985e-02,  1.5021e-01,\n",
                            "                       -2.9360e-02, -6.1159e-03,  3.2440e-02,  3.8618e-02, -2.5706e-02,\n",
                            "                        1.0383e-01, -9.2011e-02,  1.6963e-01,  6.9253e-02,  1.9840e-01,\n",
                            "                       -1.7812e-01, -2.3079e-01, -1.1263e-01, -2.4470e-01, -1.9330e-01,\n",
                            "                       -1.0708e-01, -3.7257e-03,  1.0524e-01, -1.1246e-01,  1.5298e-01,\n",
                            "                       -4.8031e-02, -8.4909e-02,  1.1196e-03, -2.3739e-02, -5.1586e-01,\n",
                            "                       -3.5094e-01,  1.3153e-01,  2.2189e-01,  1.9395e-01, -1.3074e-01,\n",
                            "                       -1.1311e-01,  8.2032e-03,  6.9414e-02,  9.0640e-02, -1.8405e-01,\n",
                            "                       -2.4364e-01, -1.8427e-01, -1.5861e-01, -1.8697e-01,  2.2595e-01,\n",
                            "                       -7.2518e-03, -1.2515e-01,  1.8512e-01, -6.6012e-02, -3.3941e-02,\n",
                            "                        2.1476e-02,  3.9181e-02, -1.3420e-01,  2.1302e-03, -2.3886e-01,\n",
                            "                       -2.7028e-01, -1.1588e-01,  5.6310e-02,  1.0223e-01,  2.0577e-01,\n",
                            "                       -1.5166e-02, -2.6961e-01,  2.9680e-01,  1.0114e-02, -5.6933e-02],\n",
                            "                      [ 7.0989e-02, -1.0396e-01,  2.1056e-03,  2.4257e-02, -1.0192e-01,\n",
                            "                        1.8622e-01, -2.6578e-01, -9.5638e-02, -4.0681e-03, -1.5508e-01,\n",
                            "                       -1.7113e-01,  5.2069e-02, -5.4240e-02,  7.1381e-02, -1.1118e-01,\n",
                            "                        3.6305e-02,  2.2311e-01, -1.3320e-01, -6.1681e-02, -4.5128e-02,\n",
                            "                       -2.9829e-02,  7.8080e-02,  5.6536e-03, -8.7470e-03, -5.6528e-02,\n",
                            "                        9.1711e-02,  2.2894e-01,  5.8631e-02,  3.1541e-02,  1.4001e-01,\n",
                            "                        1.5379e-01,  1.4542e-01,  9.5031e-03,  1.6340e-01, -2.6978e-01,\n",
                            "                       -2.6220e-01, -9.5309e-02, -3.8193e-02,  4.6011e-02, -1.3894e-01,\n",
                            "                       -1.1915e-01,  1.1227e-01, -1.3007e-01,  2.3328e-01,  2.8597e-02,\n",
                            "                       -2.8069e-01, -1.5263e-01,  1.0232e-01,  1.8737e-01, -9.2952e-02,\n",
                            "                        1.6721e-01,  9.6888e-02,  7.3461e-02,  8.4357e-04, -7.9844e-02,\n",
                            "                       -2.5419e-01,  2.1805e-02,  9.2068e-02, -1.1771e-01,  2.4425e-01,\n",
                            "                        9.2399e-02, -1.9989e-01,  1.1233e-01, -2.7302e-01,  4.9970e-02,\n",
                            "                       -1.6656e-02,  2.5869e-01, -4.8101e-01, -2.4246e-01,  9.9689e-02,\n",
                            "                        1.4623e-01,  3.3739e-01,  1.9149e-01,  1.6411e-01, -2.7309e-02,\n",
                            "                       -1.2431e-01,  2.0693e-02, -1.9503e-01,  1.0023e-01,  7.5168e-02],\n",
                            "                      [ 1.2403e-01,  1.7518e-01, -8.9457e-03, -3.5950e-02, -1.4434e-01,\n",
                            "                        2.0964e-01, -7.0636e-02, -2.1119e-01,  9.5542e-02,  4.5326e-02,\n",
                            "                       -2.0347e-01,  2.4783e-01, -1.1513e-01,  2.0143e-01, -1.3303e-01,\n",
                            "                       -1.9296e-01, -1.7739e-02, -7.8155e-02, -2.3692e-01,  2.1582e-01,\n",
                            "                       -2.1162e-01,  3.1024e-02, -1.3120e-01,  1.0371e-01, -1.0013e-01,\n",
                            "                        5.0542e-02,  1.5689e-01,  7.3048e-02,  4.8481e-04, -6.6034e-06,\n",
                            "                       -2.1825e-02, -3.0099e-02,  1.0280e-01,  8.6462e-02,  4.0059e-03,\n",
                            "                       -4.1287e-02, -1.0638e-01,  1.8790e-01, -6.0577e-02,  5.0370e-02,\n",
                            "                       -2.0490e-01, -1.7456e-01,  2.6449e-01, -1.9729e-01, -2.2626e-02,\n",
                            "                       -2.5843e-01,  7.4420e-02,  1.4078e-01, -1.0522e-01, -4.7910e-02,\n",
                            "                        9.3858e-02, -2.1932e-01, -1.4881e-01,  7.9500e-02, -7.3413e-02,\n",
                            "                        1.1199e-01, -4.0963e-02,  1.1828e-01,  1.6395e-02, -1.1792e-01,\n",
                            "                       -1.3606e-01,  9.6299e-02,  9.3126e-02,  3.8710e-01,  8.7463e-02,\n",
                            "                        7.5188e-02,  2.1383e-01,  4.8765e-02, -1.7166e-01, -6.1886e-02,\n",
                            "                        8.4205e-02, -1.7947e-01,  3.4707e-02, -3.3117e-01, -8.4617e-02,\n",
                            "                       -1.1336e-01,  2.4461e-03,  4.4998e-03, -1.2446e-01, -7.5920e-03],\n",
                            "                      [-4.6788e-02, -1.6099e-01,  1.9301e-01, -5.2311e-03, -1.3176e-01,\n",
                            "                       -5.7078e-02,  2.6596e-01, -1.6460e-01, -5.7699e-02,  6.7309e-02,\n",
                            "                        3.1067e-01,  6.9803e-03,  1.0212e-01,  3.0504e-02, -2.5091e-01,\n",
                            "                        2.7206e-01,  1.7934e-01,  8.2318e-02, -8.0118e-03, -3.4785e-02,\n",
                            "                        1.7084e-01,  1.2196e-01,  5.0562e-03, -1.4714e-01, -9.3868e-02,\n",
                            "                        7.1637e-03, -3.3460e-01, -2.2284e-02, -6.6196e-02, -1.9202e-01,\n",
                            "                       -6.7227e-02,  1.2816e-01, -3.3692e-01,  2.9948e-02, -1.5710e-02,\n",
                            "                       -3.8686e-02, -4.0774e-01,  8.8474e-02,  1.5738e-01, -1.9855e-01,\n",
                            "                       -3.6138e-02,  1.0809e-01, -4.2366e-02, -4.6055e-02,  2.6315e-01,\n",
                            "                        2.0386e-02, -5.2953e-02,  6.8700e-02,  3.6770e-02,  9.7006e-02,\n",
                            "                       -7.0237e-03, -1.3690e-01, -1.3667e-01, -1.8387e-01, -1.7980e-01,\n",
                            "                       -7.6287e-02, -1.0981e-01, -1.5551e-02,  5.8713e-02,  7.5449e-02,\n",
                            "                       -2.9082e-01,  3.8402e-01,  1.4893e-02,  1.0274e-01, -9.1658e-02,\n",
                            "                        2.0339e-01,  1.0776e-01,  2.4659e-01,  1.6089e-01, -4.6309e-02,\n",
                            "                       -1.5882e-01, -6.2692e-02,  3.8592e-02, -1.0938e-01,  1.2184e-01,\n",
                            "                       -3.2827e-01, -3.6338e-01, -1.4351e-01,  9.3009e-02,  1.6838e-01],\n",
                            "                      [-3.4402e-01, -1.3032e-01, -3.6250e-02, -6.6271e-03, -1.5368e-01,\n",
                            "                       -1.7544e-01,  1.9932e-01, -1.2812e-01, -2.5765e-02,  2.2773e-02,\n",
                            "                       -2.6323e-01, -2.1854e-01, -5.4350e-02, -8.9118e-02,  2.0473e-02,\n",
                            "                        6.1045e-02, -1.2003e-01,  2.7962e-01,  4.2586e-02, -2.0598e-01,\n",
                            "                       -1.8859e-03, -1.4213e-01, -9.1026e-03, -2.0981e-01, -1.2728e-01,\n",
                            "                       -8.5053e-02,  4.0806e-02, -1.3784e-02, -4.3367e-02,  1.4882e-01,\n",
                            "                        2.9097e-02, -9.0119e-04,  1.5006e-01,  1.0757e-01,  1.9298e-01,\n",
                            "                       -1.4689e-01,  6.4694e-02,  1.8041e-01, -3.6799e-02, -1.4236e-01,\n",
                            "                        2.1410e-01,  2.1178e-01,  1.6364e-01, -1.2282e-02, -2.6232e-01,\n",
                            "                        1.1010e-01,  1.0020e-01, -9.9893e-02,  3.5063e-02, -5.6147e-02,\n",
                            "                       -1.4096e-01, -5.9050e-02, -2.0349e-01, -5.5683e-02,  1.4740e-02,\n",
                            "                       -1.0578e-03,  2.3139e-02,  1.4731e-01,  5.7824e-02,  1.0640e-01,\n",
                            "                       -1.0130e-02, -2.6939e-02,  2.1229e-01,  1.0067e-02,  1.5309e-01,\n",
                            "                        2.4870e-01, -1.1152e-01,  5.3211e-02,  5.8904e-03,  1.6514e-01,\n",
                            "                        1.5067e-01, -6.3333e-02, -2.1352e-01,  1.2118e-02,  1.3672e-01,\n",
                            "                       -3.0359e-01,  1.1343e-01,  1.6490e-01,  1.5493e-01,  4.8332e-02],\n",
                            "                      [-2.1543e-01, -1.9715e-01, -3.3879e-01, -1.1922e-01, -1.3568e-01,\n",
                            "                        1.4276e-02,  1.0419e-01, -1.0499e-01,  1.2679e-01, -1.4657e-01,\n",
                            "                        2.0705e-01, -2.5003e-03, -2.0943e-02, -2.1419e-01,  2.9798e-01,\n",
                            "                        1.2875e-01, -7.7599e-02,  4.7192e-01, -1.4047e-01, -1.1642e-01,\n",
                            "                       -2.8786e-01, -1.2613e-01, -1.2066e-01,  4.7373e-02, -2.3803e-01,\n",
                            "                       -6.0579e-02, -1.3191e-01,  3.3770e-02,  6.3437e-02,  2.8035e-01,\n",
                            "                        4.1542e-02, -2.8765e-01, -5.3545e-02, -5.1920e-02,  1.6512e-01,\n",
                            "                        8.2617e-02,  5.3830e-02, -2.3592e-01, -2.0976e-02, -1.9161e-01,\n",
                            "                        6.7665e-02,  3.0504e-02, -1.7606e-01, -6.8108e-02, -6.0337e-02,\n",
                            "                       -1.6460e-02, -1.7820e-02,  7.2820e-02, -7.7315e-02,  1.5112e-01,\n",
                            "                        8.9001e-02, -4.2213e-02,  4.7921e-01, -1.8010e-01, -5.0718e-02,\n",
                            "                       -1.0311e-01,  4.0462e-02,  1.3174e-01,  1.6446e-01, -9.6916e-02,\n",
                            "                       -5.5939e-02, -1.7564e-01, -1.8387e-01,  1.0337e-01, -3.0234e-02,\n",
                            "                        2.2648e-01, -2.0491e-02,  1.4578e-02,  1.2500e-01,  2.2282e-02,\n",
                            "                       -1.2552e-01,  1.7370e-02,  8.5086e-04,  5.8464e-02,  5.3747e-02,\n",
                            "                       -3.9950e-03,  4.9811e-02, -1.0628e-01, -3.2827e-01, -3.2617e-02],\n",
                            "                      [ 1.3609e-01, -1.3645e-01,  1.8118e-01, -2.2962e-01,  1.8254e-01,\n",
                            "                        6.4782e-02,  1.9496e-02, -2.2267e-01,  2.2893e-01, -7.2026e-02,\n",
                            "                        7.7551e-02,  3.8332e-01,  3.1650e-02,  3.1072e-02, -2.0311e-02,\n",
                            "                        1.7597e-01,  9.9699e-03, -1.8144e-01,  2.5411e-01,  3.9608e-02,\n",
                            "                       -6.2609e-02, -1.1671e-01,  3.3713e-03, -1.2019e-01, -4.1052e-02,\n",
                            "                       -3.3368e-02,  1.9183e-03,  3.9172e-02, -1.0894e-01, -2.6052e-02,\n",
                            "                        1.6598e-01, -9.3510e-03, -4.5946e-02, -2.4041e-01,  3.8869e-01,\n",
                            "                       -6.1780e-02,  9.0888e-02, -8.6116e-03, -1.6408e-01,  2.9678e-02,\n",
                            "                       -1.3202e-01,  3.2604e-01, -7.8975e-02,  1.2783e-01,  1.0024e-01,\n",
                            "                        1.7178e-01, -1.8467e-02,  2.9529e-02, -2.4346e-02, -1.4430e-01,\n",
                            "                        9.9360e-02,  1.2490e-01,  7.7701e-02, -1.9568e-01, -1.4133e-01,\n",
                            "                       -2.4428e-02,  4.3082e-02, -2.6824e-01,  4.8093e-02, -2.3477e-01,\n",
                            "                       -9.5372e-03,  1.9468e-01, -2.3502e-01,  1.2714e-01,  5.5430e-02,\n",
                            "                       -1.6020e-01, -1.4218e-01,  1.5835e-01,  7.9282e-02, -1.3584e-01,\n",
                            "                       -1.9036e-01, -1.7410e-01,  4.0356e-02, -7.1828e-02, -2.0214e-01,\n",
                            "                        2.4889e-01,  2.4243e-01,  4.2453e-02, -8.0129e-02,  4.5087e-02]])),\n",
                            "             ('gconv1.0.self_attention_norm.weight',\n",
                            "              tensor([1.0455, 0.9354, 1.0610, 0.9520, 0.8835, 1.0192, 0.9545, 1.0328, 0.9710,\n",
                            "                      1.0702, 0.9207, 1.0346, 0.9568, 0.9609, 1.0056, 0.9926, 0.9538, 1.0957,\n",
                            "                      0.9731, 0.9418, 1.0091, 0.9760, 1.1236, 0.9876, 1.1204, 0.9513, 0.9612,\n",
                            "                      0.8958, 1.0960, 0.9115, 0.9288, 0.9962, 0.8907, 0.9494, 0.8768, 0.9538,\n",
                            "                      0.9906, 1.0853, 0.9882, 0.9889, 1.0437, 0.8707, 0.8971, 1.0014, 0.9117,\n",
                            "                      0.9180, 0.9303, 0.9606, 0.9470, 0.9401, 0.9374, 1.0142, 0.9077, 0.9666,\n",
                            "                      0.9157, 0.9211, 0.8887, 0.9688, 1.0296, 0.9383, 0.9612, 0.9804, 0.9844,\n",
                            "                      0.9532, 1.1334, 1.0838, 0.9764, 0.9827, 0.9481, 0.9750, 0.9588, 0.9795,\n",
                            "                      1.0043, 1.0360, 0.9517, 0.8974, 0.9221, 0.9238, 0.9815, 1.0308])),\n",
                            "             ('gconv1.0.self_attention_norm.bias',\n",
                            "              tensor([ 0.0524, -0.0776,  0.0640, -0.0481,  0.0222, -0.0290,  0.0471,  0.0689,\n",
                            "                      -0.0006,  0.0148, -0.0822, -0.0022,  0.0271, -0.0304,  0.0085, -0.0370,\n",
                            "                      -0.0464,  0.0572, -0.0615, -0.0790,  0.0272,  0.0357,  0.0571, -0.0103,\n",
                            "                      -0.0200,  0.0032, -0.0562,  0.1145,  0.0273,  0.0562, -0.0165,  0.0260,\n",
                            "                      -0.0745,  0.0207, -0.0558, -0.0406, -0.0137, -0.0802, -0.0012,  0.0106,\n",
                            "                       0.0068, -0.0400,  0.0234, -0.0253, -0.0658,  0.0257, -0.0032,  0.0263,\n",
                            "                      -0.0891, -0.0099, -0.1022,  0.0492, -0.0296, -0.0539, -0.0271, -0.0336,\n",
                            "                      -0.0056,  0.0046,  0.0309,  0.0605, -0.0037, -0.0645,  0.0284, -0.0965,\n",
                            "                      -0.0271, -0.0152, -0.0467,  0.0090,  0.0144, -0.0562, -0.0861,  0.0479,\n",
                            "                      -0.0421, -0.0330,  0.0116, -0.0684, -0.1076,  0.1100, -0.0134, -0.1023])),\n",
                            "             ('gconv1.0.self_attention_norm.mean_scale',\n",
                            "              tensor([0.9617, 0.9449, 0.9338, 0.8416, 0.9754, 1.0728, 1.0175, 1.0100, 0.9971,\n",
                            "                      1.0748, 0.8366, 1.0364, 0.9973, 0.9504, 0.9828, 1.0147, 1.0876, 1.0545,\n",
                            "                      1.0855, 1.1691, 0.9929, 1.0265, 1.0224, 0.9615, 0.9764, 0.9893, 0.9047,\n",
                            "                      0.9475, 1.0355, 0.9445, 1.0337, 1.0151, 0.9628, 1.0715, 0.9343, 0.9684,\n",
                            "                      1.0367, 1.0885, 1.0819, 1.0884, 1.0397, 0.9326, 1.0692, 1.0653, 0.9785,\n",
                            "                      1.0000, 0.9585, 0.9313, 1.0585, 0.9680, 1.0826, 0.9860, 1.0646, 1.0490,\n",
                            "                      0.9403, 0.9003, 1.0832, 1.0300, 1.2983, 0.8656, 0.8900, 1.0136, 1.0180,\n",
                            "                      0.8886, 0.9989, 1.0617, 1.0370, 1.0589, 1.0629, 0.9105, 1.0622, 0.9690,\n",
                            "                      1.0390, 0.9833, 1.0088, 1.0282, 1.0224, 1.0261, 0.9602, 1.1182])),\n",
                            "             ('gconv1.0.self_attention.linear_q.weight',\n",
                            "              tensor([[ 0.0915, -0.1748,  0.0941,  ..., -0.0889, -0.0732, -0.0184],\n",
                            "                      [-0.1880, -0.0414,  0.2623,  ...,  0.0085, -0.0924, -0.1879],\n",
                            "                      [-0.1389, -0.0687,  0.0495,  ...,  0.0269,  0.0513, -0.0799],\n",
                            "                      ...,\n",
                            "                      [-0.0930, -0.0311,  0.0119,  ...,  0.0114,  0.0164, -0.1862],\n",
                            "                      [-0.0005, -0.0102, -0.2764,  ..., -0.0208, -0.1332, -0.0140],\n",
                            "                      [ 0.2705, -0.0893, -0.1428,  ..., -0.0840, -0.0210, -0.0112]])),\n",
                            "             ('gconv1.0.self_attention.linear_q.bias',\n",
                            "              tensor([ 0.0445,  0.0672,  0.0132, -0.0928, -0.0968,  0.0017, -0.0462,  0.0799,\n",
                            "                       0.1561,  0.1065, -0.1068, -0.0668,  0.0600,  0.0784, -0.0575,  0.0397,\n",
                            "                      -0.0472,  0.0211,  0.0626, -0.0494,  0.0995,  0.1128,  0.0744,  0.0236,\n",
                            "                       0.0602,  0.0448, -0.1433,  0.0509, -0.0890,  0.0701,  0.0010, -0.0339,\n",
                            "                      -0.0675, -0.0543,  0.1166,  0.0186, -0.0741,  0.1143,  0.1045, -0.0624,\n",
                            "                      -0.1196,  0.0902, -0.0229, -0.0363, -0.1459, -0.0304, -0.1168,  0.0526,\n",
                            "                       0.0341,  0.0269, -0.0073, -0.0227, -0.1143, -0.1284,  0.0458, -0.1599,\n",
                            "                       0.0408, -0.1030, -0.0257, -0.0687,  0.1715, -0.1572, -0.1180,  0.0791,\n",
                            "                       0.1884,  0.0084,  0.0236, -0.0836, -0.1457, -0.0365, -0.0324, -0.1108,\n",
                            "                       0.1353,  0.0344,  0.0376,  0.0909, -0.0144,  0.0059, -0.1346,  0.1075])),\n",
                            "             ('gconv1.0.self_attention.linear_k.weight',\n",
                            "              tensor([[ 0.1338, -0.0051, -0.0995,  ..., -0.0670, -0.0554, -0.0517],\n",
                            "                      [-0.0963, -0.1749, -0.1513,  ..., -0.0077, -0.0017,  0.0429],\n",
                            "                      [ 0.1208, -0.0327,  0.0492,  ...,  0.1057, -0.1609, -0.0525],\n",
                            "                      ...,\n",
                            "                      [-0.0279,  0.0603,  0.1751,  ..., -0.2260,  0.0159, -0.3236],\n",
                            "                      [-0.0179,  0.0198, -0.0697,  ...,  0.0964, -0.1396,  0.1111],\n",
                            "                      [-0.0976, -0.1183,  0.1660,  ...,  0.0703,  0.1337,  0.0494]])),\n",
                            "             ('gconv1.0.self_attention.linear_k.bias',\n",
                            "              tensor([ 4.9961e-02,  6.0353e-02, -7.9665e-02, -7.1411e-02,  2.8687e-02,\n",
                            "                      -1.5013e-01,  1.7879e-02,  9.2881e-02, -5.2309e-02, -3.0334e-02,\n",
                            "                       8.1481e-02,  1.5015e-01, -1.6766e-03, -3.4407e-02,  7.2489e-02,\n",
                            "                       8.4918e-02,  2.9257e-03,  4.4802e-02, -1.2771e-02, -1.0266e-01,\n",
                            "                       8.7051e-02, -4.9770e-02,  7.6656e-02,  3.7917e-02, -1.0822e-01,\n",
                            "                      -5.6885e-02,  1.8294e-02, -6.6510e-02,  2.8407e-02, -1.1710e-01,\n",
                            "                       9.8490e-02,  1.8380e-02, -3.8774e-02,  9.7376e-05, -9.3777e-02,\n",
                            "                      -4.7949e-02, -5.9512e-02,  1.4891e-01, -4.0707e-03, -3.6514e-02,\n",
                            "                      -8.8810e-03, -1.6532e-03,  2.2196e-02,  1.1834e-01,  4.8630e-02,\n",
                            "                      -5.8904e-02, -6.5067e-02, -1.1941e-01, -5.4620e-02, -6.9815e-02,\n",
                            "                      -3.2176e-02, -8.8873e-02, -5.9948e-02,  8.4840e-02,  8.8949e-02,\n",
                            "                       3.4010e-02,  3.9624e-02,  6.0019e-02,  1.9420e-02,  2.9342e-02,\n",
                            "                      -2.7629e-02, -7.8930e-02,  8.6064e-02, -1.2241e-01, -8.5274e-02,\n",
                            "                      -1.0089e-01, -3.7236e-02,  1.5135e-01,  3.1541e-02,  6.9473e-03,\n",
                            "                       6.7938e-02,  1.0967e-01,  7.0865e-02, -1.4631e-02, -1.7541e-02,\n",
                            "                      -6.3626e-03,  4.9819e-02, -9.3411e-02, -5.8919e-02, -1.3453e-02])),\n",
                            "             ('gconv1.0.self_attention.linear_v.weight',\n",
                            "              tensor([[-0.3035,  0.0749,  0.0436,  ...,  0.0412,  0.0064, -0.0795],\n",
                            "                      [-0.0366,  0.0277, -0.2367,  ...,  0.1069, -0.0832,  0.0870],\n",
                            "                      [-0.0234,  0.0139,  0.0947,  ...,  0.1102,  0.0669, -0.2019],\n",
                            "                      ...,\n",
                            "                      [-0.0689, -0.0843,  0.1934,  ..., -0.0644, -0.0887, -0.1156],\n",
                            "                      [ 0.0990, -0.0945, -0.0439,  ..., -0.0715, -0.0057, -0.0209],\n",
                            "                      [ 0.1561, -0.0125, -0.0160,  ..., -0.0480,  0.0778,  0.0927]])),\n",
                            "             ('gconv1.0.self_attention.linear_v.bias',\n",
                            "              tensor([-0.0630,  0.0258, -0.0288,  0.0755, -0.1124,  0.0063, -0.0306, -0.0517,\n",
                            "                       0.1554,  0.0164, -0.0261,  0.1430,  0.0775, -0.0293, -0.1096, -0.0703,\n",
                            "                      -0.0169,  0.0933,  0.0222, -0.0197, -0.0989,  0.0627,  0.1193, -0.0346,\n",
                            "                      -0.0334, -0.0350, -0.1051,  0.0346, -0.0812,  0.0895, -0.0983,  0.1096,\n",
                            "                      -0.1425, -0.1234,  0.1034,  0.0903, -0.0619,  0.0956, -0.0636,  0.0321,\n",
                            "                       0.0722,  0.0247,  0.1330,  0.0193,  0.0372, -0.0667,  0.0213,  0.0545,\n",
                            "                      -0.0464, -0.1120, -0.0959,  0.0200, -0.0247, -0.0740,  0.0881, -0.0729,\n",
                            "                      -0.0235,  0.0110, -0.0599, -0.0182,  0.1521,  0.0230,  0.1194, -0.0569,\n",
                            "                      -0.1053,  0.0776,  0.0498,  0.1123,  0.0061, -0.1826, -0.1184,  0.1184,\n",
                            "                      -0.0365,  0.1733,  0.1081, -0.1444, -0.0138,  0.0120, -0.0669,  0.0679])),\n",
                            "             ('gconv1.0.self_attention.output_layer.weight',\n",
                            "              tensor([[ 0.1647,  0.0102, -0.1478,  ..., -0.1608,  0.3093, -0.0985],\n",
                            "                      [-0.1450,  0.1022, -0.1397,  ..., -0.1268, -0.0740,  0.1956],\n",
                            "                      [-0.0893,  0.3111,  0.1489,  ...,  0.0907,  0.0755, -0.1507],\n",
                            "                      ...,\n",
                            "                      [-0.1216, -0.0803,  0.0841,  ..., -0.0471, -0.1782, -0.0642],\n",
                            "                      [-0.0099,  0.0718,  0.0063,  ..., -0.1393, -0.0590,  0.0343],\n",
                            "                      [ 0.1225,  0.0411,  0.2176,  ...,  0.1338,  0.1469,  0.0198]])),\n",
                            "             ('gconv1.0.self_attention.output_layer.bias',\n",
                            "              tensor([-0.0956,  0.0081,  0.0666,  0.0148,  0.0376,  0.0163,  0.0237, -0.0037,\n",
                            "                       0.0489, -0.0485,  0.0335, -0.0093, -0.0312,  0.0171,  0.0325, -0.0110,\n",
                            "                      -0.0285, -0.0034,  0.0065,  0.0263,  0.0229,  0.0802, -0.0053, -0.0652,\n",
                            "                       0.0793,  0.0466, -0.0442, -0.0187,  0.0080,  0.0532,  0.0185,  0.0305,\n",
                            "                      -0.0097, -0.0671,  0.0248, -0.0351,  0.0169,  0.0781,  0.0955, -0.0592,\n",
                            "                       0.0371,  0.0625,  0.0820, -0.0071, -0.0250, -0.0014,  0.0622,  0.0393,\n",
                            "                      -0.0776, -0.0858, -0.0348,  0.0158,  0.0154,  0.0265, -0.0184,  0.0547,\n",
                            "                       0.0498,  0.0440, -0.0035,  0.0813,  0.0916,  0.0219,  0.0355, -0.0528,\n",
                            "                       0.0796,  0.0900,  0.0796, -0.0066,  0.0609, -0.0765, -0.0469,  0.0143,\n",
                            "                       0.0358,  0.0421, -0.0656, -0.0953, -0.0126,  0.0868, -0.0831,  0.0494])),\n",
                            "             ('gconv1.0.ffn_norm.weight',\n",
                            "              tensor([1.0615, 1.0025, 0.9493, 1.0058, 0.9550, 0.9044, 1.0218, 1.0664, 0.9793,\n",
                            "                      1.0378, 0.9855, 1.0111, 1.0184, 1.0495, 0.9913, 0.9990, 0.9312, 0.9567,\n",
                            "                      0.9951, 1.0101, 0.9785, 0.9961, 1.0446, 1.0079, 0.9886, 1.0095, 1.0549,\n",
                            "                      0.9400, 0.9530, 1.0155, 0.9662, 0.9730, 0.9981, 0.9566, 0.9753, 1.0117,\n",
                            "                      0.9917, 0.9711, 0.9835, 0.9327, 0.9751, 1.0253, 0.9821, 0.9029, 0.9692,\n",
                            "                      0.9749, 0.9209, 1.0062, 0.9267, 0.9775, 0.9958, 0.9012, 1.0184, 0.9409,\n",
                            "                      0.9874, 0.9638, 0.9092, 0.9910, 0.9468, 0.9523, 0.9338, 0.9256, 0.9311,\n",
                            "                      1.0770, 0.9771, 0.9929, 1.0181, 0.9923, 1.0060, 0.9993, 0.9436, 0.9517,\n",
                            "                      1.0145, 1.0007, 0.9940, 0.9388, 0.9656, 0.8939, 0.9247, 0.9451])),\n",
                            "             ('gconv1.0.ffn_norm.bias',\n",
                            "              tensor([-0.0117,  0.0346,  0.0324,  0.0403,  0.1410, -0.0079,  0.0209, -0.1574,\n",
                            "                      -0.1186,  0.0913,  0.0320, -0.0310,  0.0199,  0.0276, -0.0812,  0.0196,\n",
                            "                       0.0011,  0.0380, -0.0066,  0.0563,  0.0045, -0.0437, -0.0214, -0.0208,\n",
                            "                       0.0068,  0.0414, -0.0572,  0.0010,  0.0669, -0.0400,  0.0976, -0.0082,\n",
                            "                       0.0539, -0.0504,  0.0776, -0.0158,  0.0574,  0.1358, -0.0221, -0.0264,\n",
                            "                      -0.0944,  0.0083,  0.0250, -0.0198, -0.0436,  0.0367, -0.0406,  0.0367,\n",
                            "                       0.0498,  0.0014, -0.0060,  0.0383,  0.1049, -0.0430, -0.0151, -0.0435,\n",
                            "                       0.0488, -0.0837,  0.0088,  0.0302,  0.0971, -0.0184,  0.0620,  0.1131,\n",
                            "                      -0.0249,  0.0903, -0.0161, -0.0128,  0.0522, -0.1198, -0.0316,  0.0126,\n",
                            "                       0.0408, -0.0158,  0.0736,  0.0174, -0.0707, -0.0650, -0.0413,  0.0263])),\n",
                            "             ('gconv1.0.ffn_norm.mean_scale',\n",
                            "              tensor([0.7991, 0.8903, 0.9479, 0.9382, 0.9755, 0.8607, 0.9383, 0.8716, 0.9129,\n",
                            "                      0.9249, 1.0694, 0.8061, 1.0774, 0.9248, 0.8731, 1.0135, 0.9237, 0.9773,\n",
                            "                      1.0080, 1.0253, 0.8548, 0.9436, 0.8749, 0.9174, 0.9831, 0.9681, 1.0211,\n",
                            "                      0.9051, 0.8822, 1.0323, 0.8747, 1.0253, 0.9052, 0.8547, 1.0014, 1.0273,\n",
                            "                      1.0271, 0.8813, 1.0349, 0.9970, 0.9736, 0.9680, 1.0262, 1.0112, 0.9261,\n",
                            "                      0.9477, 0.9570, 0.8741, 0.9160, 1.0555, 0.9779, 0.8833, 1.0747, 0.9917,\n",
                            "                      0.8914, 0.8182, 0.8974, 0.9657, 0.8524, 0.9091, 1.1075, 0.9584, 0.8907,\n",
                            "                      1.1409, 0.9572, 0.8534, 0.9354, 0.9171, 0.8508, 0.9527, 0.9226, 0.9643,\n",
                            "                      1.0234, 1.0029, 0.9978, 0.9847, 0.9866, 0.9791, 0.8914, 0.9049])),\n",
                            "             ('gconv1.0.ffn.layer1.weight',\n",
                            "              tensor([[ 0.1249, -0.0589, -0.0428,  ..., -0.0298, -0.0074, -0.0676],\n",
                            "                      [-0.0809,  0.0580,  0.0808,  ..., -0.0068,  0.0311, -0.0427],\n",
                            "                      [-0.0227, -0.0955,  0.0486,  ...,  0.0088,  0.0386, -0.0236],\n",
                            "                      ...,\n",
                            "                      [-0.0547,  0.0492, -0.0109,  ...,  0.0656,  0.0202,  0.0603],\n",
                            "                      [-0.1945,  0.1779, -0.1441,  ..., -0.0019, -0.0752,  0.0403],\n",
                            "                      [ 0.0827, -0.1823, -0.0401,  ...,  0.1622, -0.1525, -0.0716]])),\n",
                            "             ('gconv1.0.ffn.layer1.bias',\n",
                            "              tensor([-7.5034e-03,  1.0462e-02, -3.8392e-02, -6.8495e-02,  2.4634e-02,\n",
                            "                       2.6599e-02, -2.6239e-02,  8.9776e-03, -9.2044e-02, -1.2637e-01,\n",
                            "                       6.2552e-02, -4.6118e-02, -1.9206e-01, -2.7917e-02, -5.4077e-02,\n",
                            "                      -9.6392e-04, -1.1383e-01, -1.2564e-01, -2.5695e-02, -8.8110e-02,\n",
                            "                      -6.3293e-04, -1.0340e-01, -1.5571e-01, -1.2112e-01,  7.1663e-02,\n",
                            "                      -3.7334e-02, -7.1262e-03, -2.5240e-02, -1.6103e-01, -2.4296e-02,\n",
                            "                      -1.5229e-01, -1.1559e-01, -2.1144e-02, -3.9009e-02,  1.7935e-02,\n",
                            "                      -5.4518e-02, -4.8799e-02, -1.3634e-01, -8.0512e-02, -2.6247e-02,\n",
                            "                      -9.9016e-02, -1.2216e-01,  1.7977e-02, -1.5230e-01, -1.1639e-01,\n",
                            "                      -5.7961e-02, -9.6630e-02, -1.4982e-01, -5.7202e-02, -1.2044e-01,\n",
                            "                      -1.0879e-01, -1.2176e-01, -9.3946e-02, -6.4083e-02, -5.5896e-02,\n",
                            "                      -3.7435e-02, -1.9716e-02,  3.2750e-04, -8.5762e-02, -9.5111e-02,\n",
                            "                      -8.9131e-02,  1.0135e-02, -3.4393e-02, -8.4909e-02, -1.0256e-01,\n",
                            "                      -1.5324e-01,  2.3903e-02, -2.4689e-02, -1.4843e-01, -6.7301e-02,\n",
                            "                       2.5398e-02, -1.0031e-01, -3.0012e-02, -3.9109e-02, -7.2780e-02,\n",
                            "                      -5.6265e-03,  1.1457e-02,  4.6992e-02, -9.1317e-02,  4.4686e-02,\n",
                            "                      -2.8117e-03, -1.2600e-01, -5.0515e-02, -7.9370e-02,  5.9218e-03,\n",
                            "                      -1.1527e-01, -4.6752e-02, -4.2448e-03, -4.5932e-02, -8.1861e-02,\n",
                            "                      -9.7721e-02, -1.1470e-01, -4.9910e-02, -5.6490e-02, -1.6516e-01,\n",
                            "                       3.3177e-02, -4.8219e-02, -1.7080e-01, -1.7612e-01, -9.5837e-02,\n",
                            "                      -1.3749e-01, -1.1530e-01, -1.3214e-01, -1.1065e-01, -5.6004e-02,\n",
                            "                       4.5524e-02, -2.1782e-02, -1.6175e-01, -7.6530e-02, -1.0208e-01,\n",
                            "                      -9.7271e-02, -5.3333e-02, -6.7415e-02,  3.3117e-02,  2.4064e-03,\n",
                            "                       2.8438e-02, -1.9140e-01, -6.0164e-02, -1.5085e-01, -1.1878e-01,\n",
                            "                      -1.5360e-01, -1.1022e-01, -1.0228e-01, -7.9359e-02,  3.7827e-03,\n",
                            "                       3.1767e-02, -1.2859e-01, -8.8060e-03, -1.2153e-01,  2.8384e-02,\n",
                            "                      -5.8378e-02, -1.1701e-01, -1.3751e-01,  2.0804e-02, -9.9984e-02,\n",
                            "                       4.2656e-02, -1.0703e-01, -3.7127e-02,  2.2106e-02,  4.2570e-02,\n",
                            "                       2.7131e-02,  2.7668e-02,  1.9206e-04,  4.7410e-02, -2.4880e-02,\n",
                            "                      -1.2813e-01, -1.9838e-02, -1.4471e-01, -4.4583e-02, -7.7560e-03,\n",
                            "                      -5.6197e-02, -1.7283e-01, -1.0201e-01, -1.1354e-01, -1.7181e-02,\n",
                            "                      -5.0328e-02, -1.5358e-01, -1.3715e-01, -4.3073e-02, -7.3742e-02,\n",
                            "                      -5.0942e-02,  7.1437e-02, -8.4279e-02, -6.4078e-03,  2.6723e-03,\n",
                            "                      -1.0893e-02,  1.4396e-02, -1.5960e-01, -1.6271e-02, -9.7636e-02,\n",
                            "                      -7.0881e-02, -1.5505e-01, -1.4936e-01, -1.5681e-01,  4.8155e-02,\n",
                            "                      -9.2613e-02, -1.5654e-01,  1.1446e-02,  3.9793e-02, -1.4097e-01,\n",
                            "                      -1.9899e-01,  2.7076e-02, -5.1030e-02, -3.0901e-02,  8.5870e-04,\n",
                            "                       1.5670e-02, -1.5147e-01, -1.3575e-02, -4.3202e-02,  6.6248e-03,\n",
                            "                      -1.7604e-01,  3.0017e-02, -3.3333e-02, -4.5215e-02, -3.9888e-02,\n",
                            "                      -3.4881e-02, -9.8738e-02, -1.4530e-01, -1.0692e-01,  4.7117e-02,\n",
                            "                      -4.1586e-02, -1.1519e-01, -7.0304e-02, -1.3577e-01, -2.5745e-02,\n",
                            "                      -5.5966e-02,  2.4004e-02, -1.0350e-01,  3.2570e-02,  5.6346e-02,\n",
                            "                      -1.2284e-01, -8.9804e-02, -4.0952e-02, -1.1940e-01, -6.0566e-02,\n",
                            "                       4.5825e-02,  1.4085e-02, -1.3805e-01, -6.6285e-02, -4.5439e-03,\n",
                            "                       2.2947e-02,  2.9507e-02, -1.3915e-03, -8.4530e-02, -8.2880e-02,\n",
                            "                       1.0405e-01,  1.0527e-01, -2.5716e-02, -2.2928e-02, -1.7231e-01,\n",
                            "                      -4.9959e-02,  4.5982e-02, -1.4600e-01, -1.2062e-01, -1.3733e-02,\n",
                            "                      -1.0118e-01,  5.7751e-02, -9.1439e-02, -1.1944e-01, -6.9092e-02,\n",
                            "                       3.1193e-02, -6.9889e-02, -1.2807e-01, -1.2232e-01,  3.3457e-02,\n",
                            "                      -1.3053e-01, -1.6249e-01,  7.1441e-02, -1.8256e-02, -8.3442e-02,\n",
                            "                      -2.9742e-02, -1.3206e-01,  8.5350e-02, -7.2303e-02, -6.3354e-03,\n",
                            "                      -6.0667e-02, -1.9181e-01, -3.4422e-03, -3.7897e-02, -1.2550e-01,\n",
                            "                      -6.6029e-02, -9.5113e-02, -8.7890e-02,  4.0699e-02,  8.6837e-02,\n",
                            "                      -6.9461e-03,  5.9471e-02, -2.1371e-02,  2.3847e-02, -1.7011e-02,\n",
                            "                       2.8531e-02, -2.9347e-02,  7.7604e-03, -6.2276e-02, -1.6017e-01,\n",
                            "                       6.0894e-03, -8.7312e-02,  3.7119e-02,  6.5375e-02, -1.1191e-01])),\n",
                            "             ('gconv1.0.ffn.layer2.weight',\n",
                            "              tensor([[ 0.0320,  0.0288, -0.0450,  ...,  0.0843, -0.0268,  0.1197],\n",
                            "                      [ 0.0858,  0.0403,  0.1470,  ..., -0.1720,  0.0796,  0.1568],\n",
                            "                      [ 0.0373, -0.0287, -0.0307,  ..., -0.1012, -0.0403, -0.0091],\n",
                            "                      ...,\n",
                            "                      [-0.0966, -0.0592, -0.1223,  ..., -0.0512,  0.0648,  0.1175],\n",
                            "                      [-0.0218, -0.0182,  0.0833,  ..., -0.0708, -0.0424, -0.0270],\n",
                            "                      [-0.0649, -0.0102, -0.0641,  ..., -0.0348,  0.0362, -0.0313]])),\n",
                            "             ('gconv1.0.ffn.layer2.bias',\n",
                            "              tensor([-0.0203,  0.0147,  0.0002,  0.0498, -0.0143,  0.0315,  0.0373, -0.0067,\n",
                            "                      -0.0836, -0.0121, -0.0217, -0.0198, -0.0489, -0.0008,  0.0894, -0.0524,\n",
                            "                      -0.0067,  0.0627,  0.0545, -0.0728,  0.0037,  0.0427, -0.0075, -0.0673,\n",
                            "                       0.0144, -0.0030,  0.0074, -0.0217,  0.1020,  0.0380, -0.0775,  0.0355,\n",
                            "                      -0.0265, -0.0066, -0.0581,  0.0898,  0.0034, -0.0079,  0.0190, -0.0013,\n",
                            "                       0.0131,  0.0147,  0.0509, -0.0718, -0.0567,  0.0017, -0.0184, -0.0278,\n",
                            "                      -0.0341,  0.0468, -0.0182, -0.0282, -0.0511,  0.0454, -0.0537,  0.0149,\n",
                            "                       0.0055, -0.0219,  0.0171, -0.0073, -0.0835,  0.0501,  0.0207,  0.0253,\n",
                            "                      -0.0919,  0.0046,  0.0069,  0.0158, -0.0114,  0.0335, -0.0480, -0.0752,\n",
                            "                       0.0318,  0.0152,  0.0282,  0.0584,  0.0149,  0.0689, -0.0223,  0.0184])),\n",
                            "             ('gconv1.1.self_attention_norm.weight',\n",
                            "              tensor([0.8511, 0.9761, 0.9121, 0.9130, 0.9071, 0.9394, 0.8398, 0.8776, 0.9834,\n",
                            "                      0.9038, 0.8832, 0.9145, 0.8974, 0.8850, 0.9131, 0.8772, 0.9420, 0.9427,\n",
                            "                      0.9396, 0.9573, 0.9419, 0.9375, 0.9029, 0.9366, 0.8921, 0.9371, 0.9173,\n",
                            "                      0.9209, 0.9411, 0.9664, 0.8936, 0.9405, 0.9232, 0.8310, 0.8623, 0.9083,\n",
                            "                      0.9557, 0.8838, 0.8837, 0.9708, 0.9371, 0.9473, 0.8354, 0.8856, 0.8993,\n",
                            "                      0.9554, 0.9952, 0.9685, 0.9088, 0.8843, 0.9010, 0.8741, 0.9222, 0.9239,\n",
                            "                      0.9197, 0.9214, 0.9225, 0.9355, 0.9321, 0.9498, 0.8794, 0.9656, 0.9753,\n",
                            "                      0.9886, 0.8909, 0.9252, 0.9205, 0.9455, 0.9044, 0.9387, 0.8869, 0.8940,\n",
                            "                      0.9293, 0.8814, 0.9431, 0.8950, 0.8458, 0.8893, 0.8953, 0.8946])),\n",
                            "             ('gconv1.1.self_attention_norm.bias',\n",
                            "              tensor([-0.0363, -0.0244,  0.0368,  0.0624, -0.0154, -0.0049,  0.0085,  0.0812,\n",
                            "                       0.0639,  0.0006,  0.0471, -0.0026,  0.0019, -0.0291, -0.0058,  0.0124,\n",
                            "                      -0.0129,  0.0023, -0.0140, -0.0118,  0.0162, -0.0278, -0.0067, -0.0067,\n",
                            "                       0.0256, -0.0083, -0.0074, -0.0305, -0.0146, -0.0312, -0.0068,  0.0179,\n",
                            "                       0.0061,  0.0350,  0.0095, -0.0472, -0.0390, -0.0157,  0.0212, -0.0212,\n",
                            "                       0.0527,  0.0227,  0.0096, -0.0628, -0.0098, -0.0361, -0.0234,  0.0193,\n",
                            "                       0.0781,  0.0425, -0.0916,  0.0189, -0.0174,  0.0499, -0.0166, -0.0062,\n",
                            "                       0.0373, -0.0505, -0.0157, -0.0377,  0.0169,  0.0458,  0.0428, -0.0249,\n",
                            "                      -0.0003, -0.0686, -0.0229,  0.0291, -0.0243, -0.0060, -0.0323,  0.0305,\n",
                            "                       0.0693, -0.0071, -0.0231,  0.0170, -0.0130,  0.0697,  0.0063,  0.0720])),\n",
                            "             ('gconv1.1.self_attention_norm.mean_scale',\n",
                            "              tensor([1.0001, 1.0241, 1.0384, 1.0056, 1.0984, 1.0520, 0.9964, 1.0302, 1.0431,\n",
                            "                      1.1159, 1.0794, 0.9964, 1.0734, 1.0420, 1.0508, 1.0121, 0.9779, 1.0251,\n",
                            "                      1.0514, 1.0835, 0.9591, 0.9994, 1.0241, 0.9983, 1.0109, 1.0487, 1.0919,\n",
                            "                      1.0543, 0.9392, 0.9883, 1.0150, 0.9247, 1.0550, 0.9293, 1.0919, 1.0912,\n",
                            "                      1.0023, 0.9960, 1.0749, 1.2002, 1.0270, 1.1859, 1.0362, 0.9531, 1.0209,\n",
                            "                      1.0279, 1.0569, 1.0542, 0.9229, 1.0063, 1.0590, 0.9811, 1.0100, 1.0460,\n",
                            "                      0.9874, 1.0052, 1.0183, 1.0750, 1.0295, 0.9453, 1.0830, 1.0469, 1.2107,\n",
                            "                      1.0999, 1.0140, 1.0776, 0.8901, 1.0210, 1.0047, 1.0589, 1.1158, 0.9440,\n",
                            "                      1.0342, 1.0367, 0.9655, 1.0616, 1.0197, 0.9189, 1.0561, 1.0356])),\n",
                            "             ('gconv1.1.self_attention.linear_q.weight',\n",
                            "              tensor([[ 0.0193,  0.0838,  0.0280,  ..., -0.0437,  0.0319,  0.0561],\n",
                            "                      [ 0.0214, -0.2270,  0.0089,  ...,  0.1819, -0.0600,  0.0931],\n",
                            "                      [-0.0186,  0.2383, -0.0203,  ...,  0.2440,  0.0230,  0.0033],\n",
                            "                      ...,\n",
                            "                      [-0.0375,  0.0683,  0.1144,  ...,  0.0656,  0.0098, -0.0135],\n",
                            "                      [-0.1739, -0.1036, -0.0674,  ...,  0.0476,  0.3316,  0.0481],\n",
                            "                      [ 0.1050,  0.0344,  0.0081,  ..., -0.1360, -0.0726,  0.0902]])),\n",
                            "             ('gconv1.1.self_attention.linear_q.bias',\n",
                            "              tensor([-0.1169,  0.0994,  0.0049,  0.0462,  0.0365,  0.0191,  0.0230,  0.1095,\n",
                            "                       0.0866, -0.0885,  0.0541, -0.0410, -0.0850,  0.0108,  0.0541, -0.0060,\n",
                            "                       0.0890,  0.0689,  0.0396, -0.0918,  0.0138, -0.1103,  0.0521, -0.0585,\n",
                            "                      -0.0058, -0.0418,  0.0170,  0.0144, -0.1477,  0.0641,  0.0157, -0.0207,\n",
                            "                       0.0793, -0.0860,  0.0265,  0.1265, -0.0196,  0.1573,  0.0686,  0.0586,\n",
                            "                      -0.1278, -0.0137,  0.0216, -0.1240,  0.0198, -0.0363, -0.0008,  0.0611,\n",
                            "                       0.0558, -0.1138, -0.0856,  0.0811,  0.0598, -0.0613, -0.0853,  0.0335,\n",
                            "                      -0.1397,  0.0014, -0.0503, -0.0458,  0.0591, -0.0758, -0.0566, -0.0887,\n",
                            "                       0.0833, -0.0610,  0.1177, -0.0886,  0.0581, -0.0225, -0.0128, -0.0888,\n",
                            "                      -0.0407,  0.0472,  0.0901, -0.0294, -0.0096,  0.0181,  0.0681,  0.0217])),\n",
                            "             ('gconv1.1.self_attention.linear_k.weight',\n",
                            "              tensor([[-0.0198,  0.1179,  0.0539,  ...,  0.0224,  0.0076, -0.0206],\n",
                            "                      [ 0.0654, -0.1262,  0.0936,  ..., -0.1562,  0.1319,  0.0808],\n",
                            "                      [-0.1047,  0.1071, -0.1146,  ...,  0.0767, -0.0496,  0.1945],\n",
                            "                      ...,\n",
                            "                      [-0.0062, -0.0283,  0.1149,  ...,  0.0064, -0.1088, -0.0328],\n",
                            "                      [ 0.1244, -0.1291, -0.1351,  ...,  0.1301,  0.0549,  0.0373],\n",
                            "                      [ 0.1447,  0.0434,  0.1901,  ..., -0.0323, -0.1406, -0.0488]])),\n",
                            "             ('gconv1.1.self_attention.linear_k.bias',\n",
                            "              tensor([ 0.0606,  0.0542, -0.0826, -0.0504,  0.0783, -0.0341, -0.0891, -0.0776,\n",
                            "                       0.1086, -0.0963,  0.0054, -0.0261, -0.0817, -0.0827,  0.1013,  0.0630,\n",
                            "                      -0.0460,  0.0641, -0.1077, -0.0430,  0.0053, -0.1101, -0.0833, -0.0648,\n",
                            "                       0.0692,  0.0631, -0.0996,  0.0376, -0.1029, -0.0222, -0.0938,  0.0033,\n",
                            "                       0.0896, -0.0664, -0.0131, -0.0238, -0.0262, -0.0786,  0.0638,  0.0120,\n",
                            "                       0.0250,  0.0637, -0.0349,  0.0655,  0.0689,  0.0869,  0.0890,  0.0448,\n",
                            "                      -0.0557,  0.0113, -0.0784,  0.0020, -0.0780,  0.0204, -0.0661, -0.0332,\n",
                            "                       0.0204, -0.0159, -0.0784, -0.0811, -0.1004,  0.0210, -0.1094, -0.0283,\n",
                            "                       0.0743, -0.0003,  0.0594, -0.0598, -0.0370, -0.0929, -0.0896, -0.0798,\n",
                            "                      -0.0738, -0.0156, -0.0239,  0.1043, -0.0288,  0.0638, -0.0785,  0.0902])),\n",
                            "             ('gconv1.1.self_attention.linear_v.weight',\n",
                            "              tensor([[-0.0959, -0.0067,  0.0517,  ..., -0.0776,  0.0355, -0.0923],\n",
                            "                      [-0.0971,  0.1924, -0.0853,  ...,  0.2049, -0.1403, -0.0303],\n",
                            "                      [ 0.0331,  0.1403, -0.0007,  ..., -0.0305, -0.0036,  0.1985],\n",
                            "                      ...,\n",
                            "                      [ 0.0110,  0.1585, -0.0157,  ..., -0.1857, -0.1438,  0.1125],\n",
                            "                      [-0.2297,  0.0950, -0.0893,  ..., -0.1031,  0.2138, -0.3100],\n",
                            "                      [ 0.1014,  0.0044, -0.2046,  ..., -0.1068, -0.0917, -0.1091]])),\n",
                            "             ('gconv1.1.self_attention.linear_v.bias',\n",
                            "              tensor([-0.0383,  0.1737, -0.0235, -0.0066,  0.0365,  0.0973, -0.0695,  0.0888,\n",
                            "                       0.0561,  0.0818, -0.1105,  0.0769, -0.0665, -0.1241,  0.1201, -0.0796,\n",
                            "                      -0.0353,  0.0646, -0.0181,  0.0152, -0.0627, -0.1449,  0.0759, -0.0017,\n",
                            "                      -0.0621, -0.1592, -0.0112, -0.0693, -0.0523, -0.0436, -0.0716, -0.0872,\n",
                            "                      -0.0275, -0.1202, -0.0557, -0.0731, -0.1145, -0.0398,  0.0167, -0.0336,\n",
                            "                      -0.0501, -0.0887,  0.1384,  0.0202, -0.0882, -0.0011,  0.0661,  0.0492,\n",
                            "                       0.1043, -0.1035, -0.1437, -0.0671, -0.0942, -0.0996,  0.0238, -0.1035,\n",
                            "                       0.0606,  0.0120, -0.0786,  0.0540, -0.0105,  0.0323, -0.1045,  0.0238,\n",
                            "                       0.0172, -0.0145, -0.1166,  0.0643, -0.0560,  0.0639, -0.1317,  0.1002,\n",
                            "                      -0.0214,  0.0455,  0.0461, -0.1433,  0.1193, -0.0748, -0.1509, -0.0400])),\n",
                            "             ('gconv1.1.self_attention.output_layer.weight',\n",
                            "              tensor([[ 0.0124, -0.1297, -0.1127,  ...,  0.0918,  0.0194,  0.0244],\n",
                            "                      [-0.0781,  0.0635, -0.0114,  ..., -0.0504, -0.0701, -0.1661],\n",
                            "                      [-0.0848, -0.2343,  0.0172,  ...,  0.1612, -0.1288, -0.1468],\n",
                            "                      ...,\n",
                            "                      [-0.1588, -0.1020, -0.0790,  ...,  0.0989, -0.1925,  0.0955],\n",
                            "                      [ 0.0077, -0.1027, -0.0228,  ...,  0.1544, -0.0105,  0.0998],\n",
                            "                      [ 0.1300, -0.1514,  0.1528,  ...,  0.0684, -0.1245, -0.0114]])),\n",
                            "             ('gconv1.1.self_attention.output_layer.bias',\n",
                            "              tensor([ 0.0692, -0.0288, -0.0284, -0.0728,  0.0213, -0.0675,  0.1268,  0.0856,\n",
                            "                       0.0456, -0.0530, -0.1155,  0.0946, -0.0441,  0.0680,  0.0823, -0.0402,\n",
                            "                      -0.0593, -0.0403,  0.0637, -0.0101, -0.0988, -0.0872, -0.0862,  0.0799,\n",
                            "                      -0.0402, -0.0113,  0.0704,  0.1432,  0.1559, -0.0298, -0.1236, -0.0597,\n",
                            "                      -0.1277, -0.0182, -0.0817,  0.1314, -0.0340, -0.0138, -0.0562,  0.0487,\n",
                            "                      -0.0918, -0.0107,  0.0064, -0.0480, -0.0480, -0.0277, -0.1443, -0.0404,\n",
                            "                       0.0375,  0.0910, -0.1400,  0.0448, -0.0397,  0.0871, -0.0853,  0.0730,\n",
                            "                      -0.0468, -0.0604, -0.0906,  0.0612, -0.1011, -0.0077,  0.0874, -0.0380,\n",
                            "                      -0.1126,  0.0892,  0.0399,  0.0833,  0.0595,  0.0762,  0.0792,  0.0126,\n",
                            "                       0.0345, -0.0471,  0.0661, -0.1073,  0.0362,  0.0316, -0.0605,  0.0105])),\n",
                            "             ('gconv1.1.ffn_norm.weight',\n",
                            "              tensor([0.9487, 0.8843, 0.9565, 0.9253, 0.9435, 0.9139, 0.9584, 0.9443, 0.9374,\n",
                            "                      0.9483, 0.9043, 0.8127, 0.9545, 0.9705, 0.8957, 0.9109, 0.8765, 0.9632,\n",
                            "                      1.0324, 1.0027, 0.8801, 0.9143, 0.9699, 0.9576, 0.9293, 0.9341, 0.9209,\n",
                            "                      0.8350, 0.8951, 0.9764, 0.9213, 0.8570, 0.9093, 0.9971, 0.9682, 0.8103,\n",
                            "                      0.8221, 0.9039, 0.9311, 0.9575, 0.9250, 0.9667, 1.0019, 0.8694, 0.8983,\n",
                            "                      0.8823, 0.8915, 1.0023, 0.9629, 0.9215, 0.9051, 0.8597, 0.9580, 0.9028,\n",
                            "                      0.8985, 0.9648, 0.9346, 0.9240, 0.9564, 0.9886, 0.9105, 0.9572, 0.9489,\n",
                            "                      0.9923, 0.9257, 0.9665, 0.9666, 0.9584, 0.9965, 0.9817, 0.9295, 0.9690,\n",
                            "                      0.8561, 0.9565, 0.9127, 0.9237, 0.9096, 0.9261, 0.9344, 0.9526])),\n",
                            "             ('gconv1.1.ffn_norm.bias',\n",
                            "              tensor([-0.0403,  0.0127, -0.0128, -0.0003,  0.0037,  0.0113,  0.0075,  0.0083,\n",
                            "                      -0.0096,  0.0010,  0.0152, -0.0107, -0.0432,  0.0042,  0.0053,  0.0164,\n",
                            "                      -0.0072, -0.0229, -0.0148,  0.0142, -0.0396, -0.0438, -0.0047, -0.0081,\n",
                            "                       0.0037,  0.0515, -0.0122,  0.0115, -0.0149, -0.0048, -0.0026, -0.0178,\n",
                            "                      -0.0260,  0.0234, -0.0532, -0.0498, -0.0235,  0.0189, -0.0280,  0.0006,\n",
                            "                       0.0579,  0.0283,  0.0021, -0.0241,  0.0009,  0.0290, -0.0283, -0.0380,\n",
                            "                       0.0315, -0.0215, -0.0077,  0.0195,  0.0258, -0.0300, -0.0338,  0.0329,\n",
                            "                       0.0119, -0.0024,  0.0418,  0.0191,  0.0295,  0.0135, -0.0176,  0.0104,\n",
                            "                      -0.0054,  0.0234, -0.0089, -0.0299, -0.0214,  0.0053, -0.0293,  0.0263,\n",
                            "                      -0.0044, -0.0132, -0.0322, -0.0077, -0.0021,  0.0110, -0.0226,  0.0112])),\n",
                            "             ('gconv1.1.ffn_norm.mean_scale',\n",
                            "              tensor([0.9221, 0.9638, 0.9183, 0.9617, 1.0879, 0.9915, 1.0298, 0.9302, 0.9705,\n",
                            "                      0.9897, 0.9912, 1.0022, 0.9243, 0.9970, 0.9975, 0.9734, 0.9612, 0.9933,\n",
                            "                      0.9999, 1.0246, 1.0050, 0.9663, 0.8995, 0.9861, 0.9629, 0.9372, 1.0018,\n",
                            "                      1.0261, 0.9906, 0.9723, 0.9694, 1.0059, 0.9561, 0.9637, 0.9784, 0.9643,\n",
                            "                      0.9377, 1.0044, 0.8621, 1.0683, 0.8947, 1.1094, 1.1073, 0.9712, 1.0005,\n",
                            "                      0.9773, 0.8836, 0.9999, 0.9845, 0.9919, 1.0295, 0.9529, 0.9529, 0.9354,\n",
                            "                      0.9413, 0.9572, 0.9565, 1.0241, 1.0160, 0.9609, 0.9400, 0.9998, 0.9805,\n",
                            "                      1.0290, 0.9934, 1.0075, 0.8607, 1.0138, 0.9891, 0.9487, 0.9512, 0.8927,\n",
                            "                      0.9209, 0.9467, 1.0216, 0.9672, 0.9985, 0.9943, 1.0051, 0.9464])),\n",
                            "             ('gconv1.1.ffn.layer1.weight',\n",
                            "              tensor([[ 0.1195,  0.0664, -0.0822,  ...,  0.0332,  0.0158,  0.0101],\n",
                            "                      [ 0.1246, -0.0725,  0.1625,  ...,  0.0400, -0.0146,  0.0308],\n",
                            "                      [ 0.1136,  0.0201,  0.0339,  ...,  0.0084, -0.1756, -0.0070],\n",
                            "                      ...,\n",
                            "                      [ 0.0597,  0.0311,  0.1214,  ...,  0.1489, -0.2087, -0.0804],\n",
                            "                      [ 0.0585,  0.1357, -0.0224,  ...,  0.0747,  0.1735,  0.0701],\n",
                            "                      [-0.0224,  0.1200,  0.0695,  ...,  0.0061,  0.0884,  0.0871]])),\n",
                            "             ('gconv1.1.ffn.layer1.bias',\n",
                            "              tensor([-1.7799e-02, -3.8203e-02,  9.7440e-02,  2.7983e-02,  3.0494e-03,\n",
                            "                      -1.1953e-01,  1.3599e-02, -1.1417e-01,  6.5668e-02, -1.4073e-01,\n",
                            "                      -9.0692e-02, -4.5942e-02, -6.2384e-03, -4.2047e-02,  2.1481e-02,\n",
                            "                      -2.4444e-02, -1.2303e-01,  7.2822e-02, -7.8847e-03, -1.1557e-01,\n",
                            "                       9.1356e-02,  9.8429e-02, -1.1916e-01, -2.1402e-02, -2.3970e-02,\n",
                            "                      -9.6395e-03,  1.1012e-01, -7.0815e-02,  4.3266e-02, -1.0278e-01,\n",
                            "                      -3.7610e-02, -1.1927e-01,  7.1729e-02, -1.8459e-02, -2.8640e-02,\n",
                            "                      -1.4813e-01, -1.0419e-01, -8.5070e-02, -7.7299e-02, -1.1705e-02,\n",
                            "                      -2.3042e-02, -1.1159e-01, -4.6257e-03, -1.1081e-01, -8.0812e-02,\n",
                            "                      -5.9280e-02, -7.8742e-02, -1.3733e-01,  1.8473e-02, -1.2619e-01,\n",
                            "                       4.8473e-02,  6.7311e-02,  3.2032e-02,  4.0944e-02, -5.9298e-02,\n",
                            "                      -1.1042e-01, -9.3048e-02,  7.7085e-02,  5.7173e-02, -7.7159e-02,\n",
                            "                      -6.5734e-02, -7.6892e-02, -1.9303e-03, -1.1017e-01,  1.4348e-02,\n",
                            "                      -5.7481e-03,  4.6552e-02, -3.5049e-02, -2.2351e-02, -6.0756e-02,\n",
                            "                      -7.2741e-02, -2.4201e-02, -8.7584e-02, -1.1188e-01, -1.2346e-01,\n",
                            "                       3.0706e-02,  5.7586e-05,  8.1372e-02, -6.8296e-02, -1.2144e-01,\n",
                            "                      -1.1554e-01, -6.9019e-02, -3.6516e-02,  4.6447e-02, -1.0053e-01,\n",
                            "                      -2.4999e-02, -1.7218e-01, -1.1533e-01, -7.5337e-02,  5.7523e-02,\n",
                            "                      -7.4721e-02, -7.7837e-02, -2.4642e-02, -6.4409e-02, -1.0843e-01,\n",
                            "                      -9.2599e-02,  6.1111e-02, -3.6296e-02, -9.9870e-02, -9.8702e-02,\n",
                            "                       4.1471e-02, -4.9048e-03, -1.1893e-01,  7.7511e-02, -1.0266e-01,\n",
                            "                       6.5721e-02,  2.4749e-02,  7.5369e-02, -1.0921e-01, -1.1458e-01,\n",
                            "                      -8.0622e-03,  3.2974e-02, -1.7936e-02, -7.8803e-02, -7.6092e-02,\n",
                            "                      -1.8626e-03, -1.6322e-01, -8.4166e-02, -6.0961e-02, -6.4430e-03,\n",
                            "                      -1.2144e-01,  3.3185e-02, -1.9382e-02, -1.2586e-01, -6.8993e-02,\n",
                            "                      -6.9906e-02, -9.6210e-02,  1.3872e-02,  1.7807e-02, -1.0654e-01,\n",
                            "                      -3.5590e-02,  8.5246e-03, -9.8732e-02,  4.3153e-02,  3.0395e-02,\n",
                            "                      -4.0850e-02, -1.1513e-01, -4.0765e-02, -7.5009e-02,  4.5547e-02,\n",
                            "                      -8.3537e-02, -4.7001e-02, -4.1366e-02, -6.8196e-02, -9.2275e-02,\n",
                            "                       3.0633e-02,  5.5470e-02, -6.7516e-02, -1.1111e-01, -1.1906e-01,\n",
                            "                      -1.1325e-01,  2.9221e-02, -3.2777e-02,  1.0192e-02, -4.1123e-02,\n",
                            "                      -1.3171e-01,  2.8402e-02, -5.1317e-02,  1.5707e-02, -4.5332e-02,\n",
                            "                       3.2145e-02, -1.2218e-01,  2.2815e-02,  6.0005e-02, -6.4617e-02,\n",
                            "                      -1.1700e-01, -3.6122e-02, -1.5018e-01, -5.9039e-02, -3.5529e-02,\n",
                            "                       3.2904e-02, -1.9449e-02, -2.2117e-02,  1.5289e-02, -2.1503e-02,\n",
                            "                       2.3213e-02, -1.0795e-01, -1.8518e-02, -1.0600e-01, -3.7382e-03,\n",
                            "                      -6.1212e-02, -4.3440e-02, -1.4534e-02, -1.0791e-01,  5.7319e-04,\n",
                            "                      -5.7019e-02,  5.8051e-02, -6.0161e-02, -3.2470e-02, -1.1893e-01,\n",
                            "                      -1.0008e-01,  9.2098e-02, -3.0486e-02, -4.0862e-02, -5.2371e-02,\n",
                            "                      -2.9197e-02, -1.0212e-01, -1.1485e-01,  3.7874e-02, -3.5332e-02,\n",
                            "                       7.8783e-02, -3.5872e-02, -7.3227e-02, -6.4017e-02,  1.0868e-03,\n",
                            "                       1.2210e-02, -1.3283e-01, -1.5643e-02, -5.8910e-02, -8.6452e-03,\n",
                            "                      -4.7190e-02, -5.6583e-02, -3.5833e-02, -2.3809e-02,  8.0595e-03,\n",
                            "                       1.8627e-02, -4.6500e-02, -1.1132e-01, -2.2425e-02, -8.4867e-02,\n",
                            "                      -3.2070e-02, -3.4263e-02, -7.3674e-02,  4.8370e-03,  6.1215e-02,\n",
                            "                      -1.4025e-02, -8.8787e-02, -5.1090e-02, -7.5359e-02, -1.0024e-01,\n",
                            "                      -5.6169e-02, -1.2133e-01, -5.6344e-02, -9.9948e-02, -5.2224e-02,\n",
                            "                       7.9210e-02, -9.5733e-02,  6.6714e-02,  4.3775e-02, -1.0864e-02,\n",
                            "                      -9.1981e-02, -2.3431e-02,  5.4084e-02, -1.1617e-02, -1.1812e-01,\n",
                            "                      -3.5174e-02, -1.2372e-01,  9.3832e-02,  2.8080e-02,  3.7506e-03,\n",
                            "                       8.1106e-02, -5.6013e-05, -7.1608e-02, -2.9469e-02,  7.7249e-03,\n",
                            "                       2.6380e-02, -3.7821e-02,  2.2617e-02,  5.0968e-02,  4.8982e-02,\n",
                            "                      -8.3472e-02, -3.1881e-02,  6.0373e-02,  7.3261e-03,  1.4744e-03,\n",
                            "                       4.2309e-03, -3.3946e-02,  3.3746e-02, -1.1485e-02,  6.0193e-02,\n",
                            "                      -5.8256e-02,  3.4946e-02, -1.0822e-02, -5.4852e-02, -9.7662e-02,\n",
                            "                      -2.5680e-02, -6.7552e-02, -1.1707e-01, -7.7148e-02, -3.0145e-02])),\n",
                            "             ('gconv1.1.ffn.layer2.weight',\n",
                            "              tensor([[ 0.1469,  0.0270, -0.0286,  ...,  0.1331,  0.0039, -0.1354],\n",
                            "                      [-0.0518,  0.0507, -0.0070,  ..., -0.0837, -0.0126,  0.0287],\n",
                            "                      [ 0.0105, -0.0398, -0.0357,  ...,  0.1023,  0.0463, -0.1144],\n",
                            "                      ...,\n",
                            "                      [ 0.0410, -0.0181,  0.0490,  ..., -0.0283,  0.0524, -0.0286],\n",
                            "                      [ 0.0722, -0.0591,  0.0742,  ...,  0.0510,  0.0383,  0.0679],\n",
                            "                      [ 0.0619, -0.0442,  0.0749,  ...,  0.0599, -0.0070, -0.0920]])),\n",
                            "             ('gconv1.1.ffn.layer2.bias',\n",
                            "              tensor([-0.0272,  0.0065, -0.0606, -0.0353, -0.0103,  0.0476,  0.0644,  0.0348,\n",
                            "                      -0.0028,  0.0627, -0.0596,  0.0006,  0.0171, -0.0542, -0.0211, -0.0145,\n",
                            "                      -0.0540,  0.0131,  0.0147, -0.0258, -0.0250, -0.0078,  0.0426,  0.0357,\n",
                            "                      -0.0032,  0.0187,  0.0188,  0.0610,  0.0608,  0.0040, -0.0441, -0.0013,\n",
                            "                      -0.0226, -0.0316,  0.0268, -0.0187,  0.0569,  0.0041,  0.0003, -0.0486,\n",
                            "                       0.0212,  0.0213,  0.0250, -0.0565, -0.0906, -0.0631, -0.0243, -0.0557,\n",
                            "                       0.0754, -0.0479, -0.0423, -0.0182, -0.0196, -0.0435, -0.0802, -0.0423,\n",
                            "                       0.0319,  0.0417,  0.0408,  0.0104, -0.0213, -0.0411,  0.0095, -0.0027,\n",
                            "                      -0.0532,  0.0121, -0.0544, -0.0663,  0.0042,  0.0518, -0.0273, -0.0260,\n",
                            "                       0.0660,  0.0506,  0.0306,  0.0334,  0.0415,  0.1108,  0.0040, -0.0003])),\n",
                            "             ('FC.0.weight',\n",
                            "              tensor([[ 0.0544, -0.0779, -0.1254,  ...,  0.0516, -0.2001,  0.1506],\n",
                            "                      [-0.0241, -0.0593, -0.1311,  ..., -0.0311,  0.1186, -0.0814],\n",
                            "                      [-0.0693, -0.0841,  0.1324,  ..., -0.1206,  0.0706,  0.0366],\n",
                            "                      ...,\n",
                            "                      [ 0.0491, -0.0370,  0.0538,  ...,  0.1625,  0.0194, -0.0233],\n",
                            "                      [-0.1503, -0.0652, -0.0728,  ..., -0.0619, -0.0254, -0.0466],\n",
                            "                      [-0.1078,  0.0173,  0.0747,  ..., -0.0086, -0.0134,  0.2396]])),\n",
                            "             ('FC.0.bias',\n",
                            "              tensor([ 0.2235,  0.0831,  0.0912,  0.3192,  0.0280,  0.2148,  0.1049, -0.1276,\n",
                            "                       0.1412,  0.1354, -0.0554,  0.0736,  0.0778,  0.2588, -0.0869,  0.4208,\n",
                            "                       0.0055,  0.5198,  0.0586,  0.0421,  0.1774,  0.4329,  0.1136,  0.1128,\n",
                            "                       0.1850,  0.1160,  0.1058,  0.1577,  0.0035,  0.0462,  0.0387, -0.0533,\n",
                            "                       0.0938,  0.1068, -0.0015,  0.3436,  0.2273,  0.1186,  0.1223,  0.1299,\n",
                            "                      -0.0135,  0.2483,  0.2227,  0.1226,  0.0459,  0.1731,  0.0787,  0.3024,\n",
                            "                       0.0562,  0.0848,  0.2466,  0.1461,  0.2643,  0.2577,  0.1684,  0.2479,\n",
                            "                       0.1796, -0.0359,  0.1011, -0.0237,  0.0062, -0.0242,  0.1712,  0.0673,\n",
                            "                      -0.0326,  0.0254,  0.2938, -0.0028,  0.0642,  0.0495,  0.0336, -0.0614,\n",
                            "                       0.3734,  0.0557, -0.0538,  0.3919,  0.3281, -0.1175,  0.2388,  0.3747,\n",
                            "                       0.3384,  0.1128, -0.0598,  0.2467, -0.0416, -0.1174,  0.0654, -0.0408,\n",
                            "                       0.0918,  0.1006,  0.3930,  0.0975, -0.0667,  0.1723, -0.0372,  0.2345,\n",
                            "                       0.2596,  0.1904,  0.0255,  0.1680,  0.2648,  0.1782,  0.0071,  0.1469,\n",
                            "                       0.1010,  0.0297, -0.0335,  0.0851,  0.0048,  0.1467, -0.0825,  0.2830,\n",
                            "                       0.1311,  0.1067,  0.0888,  0.0820,  0.2817, -0.0613,  0.0867, -0.0421,\n",
                            "                       0.0557,  0.1813,  0.0336, -0.1369,  0.1263,  0.2181,  0.3019,  0.2729])),\n",
                            "             ('FC.1.weight',\n",
                            "              tensor([[ 0.1903, -0.0207, -0.1737,  ..., -0.0346,  0.0392, -0.0149],\n",
                            "                      [ 0.0731,  0.0789, -0.0215,  ..., -0.1973, -0.0894, -0.0300],\n",
                            "                      [-0.0217,  0.0635, -0.0072,  ...,  0.1403,  0.0756,  0.1453],\n",
                            "                      ...,\n",
                            "                      [ 0.0048,  0.1419, -0.0358,  ..., -0.0623,  0.0726,  0.0865],\n",
                            "                      [-0.0573, -0.1278, -0.0762,  ..., -0.1249, -0.2423, -0.0702],\n",
                            "                      [ 0.1591,  0.0427, -0.0872,  ..., -0.0361, -0.0209, -0.0313]])),\n",
                            "             ('FC.1.bias',\n",
                            "              tensor([-1.5790e-01, -4.9348e-02, -1.2599e-01,  4.0336e-02,  4.1894e-02,\n",
                            "                       2.9317e-01,  1.6218e-01,  3.3124e-02, -1.7366e-01,  3.4195e-02,\n",
                            "                       6.6144e-02, -1.5273e-01, -1.6623e-01,  5.7704e-03,  1.0120e-01,\n",
                            "                       6.2459e-02, -1.1592e-01,  3.3587e-03, -6.3204e-02, -5.2252e-02,\n",
                            "                      -1.6755e-02,  1.6807e-02, -4.3108e-02,  2.9226e-01,  9.5733e-02,\n",
                            "                       3.7127e-01,  1.7770e-01, -4.4670e-02,  7.2144e-02, -1.2734e-01,\n",
                            "                       1.9729e-01,  1.6598e-01, -7.3874e-03, -9.0677e-02, -3.0363e-02,\n",
                            "                       1.1690e-01, -1.9199e-01, -1.1072e-01,  4.6088e-01,  5.5752e-01,\n",
                            "                       7.1030e-02, -5.4413e-02,  1.0350e-01,  9.8569e-02, -3.3810e-02,\n",
                            "                      -1.3410e-01, -1.4522e-01, -1.2693e-01,  1.5837e-01,  1.0829e-01,\n",
                            "                       8.0258e-03,  6.8269e-02,  1.8270e-01,  3.7342e-01,  5.1476e-02,\n",
                            "                       1.2083e-02,  1.5583e-01,  7.0453e-02, -1.3306e-01, -8.6319e-02,\n",
                            "                      -6.5919e-02, -5.6764e-02,  1.0952e-01, -1.9815e-01,  5.8637e-01,\n",
                            "                       1.9150e-01, -1.6601e-01,  5.8832e-02, -1.0813e-01,  2.8098e-02,\n",
                            "                       7.6239e-02,  3.2819e-01,  7.0379e-02, -1.9947e-01,  1.7643e-01,\n",
                            "                       2.7103e-01, -7.3578e-03, -4.0915e-03, -1.0161e-01,  3.7679e-03,\n",
                            "                      -2.5300e-04, -5.0962e-02, -1.0713e-02, -3.0349e-02, -8.1394e-02,\n",
                            "                       2.6305e-01,  6.6719e-02,  5.4792e-01,  1.1927e-01,  9.3484e-02,\n",
                            "                       4.8949e-01, -1.9221e-01,  3.7394e-01,  2.1156e-01,  4.5335e-02,\n",
                            "                       1.0005e-01,  8.3108e-02,  1.3269e-01,  8.2352e-02,  8.8342e-02,\n",
                            "                       6.7095e-02, -3.5221e-02, -1.1620e-01, -2.1626e-02, -2.2041e-01,\n",
                            "                      -1.2753e-02, -8.2045e-02,  5.2606e-02, -4.2681e-03,  9.0511e-02,\n",
                            "                       5.5339e-02,  3.1493e-01,  7.1612e-02, -1.5529e-01,  1.9216e-01,\n",
                            "                       3.6801e-01,  1.1548e-01,  2.1627e-01,  1.1501e-01,  2.8442e-01,\n",
                            "                       5.3127e-02,  7.1780e-02,  1.2947e-01,  1.8269e-02,  2.0401e-01,\n",
                            "                       2.6682e-01,  1.1255e-01,  1.7806e-01])),\n",
                            "             ('FC.2.weight',\n",
                            "              tensor([[-0.0650, -0.1939,  0.0932,  ..., -0.0827, -0.1083, -0.0803],\n",
                            "                      [-0.1368, -0.1511, -0.0547,  ...,  0.0440, -0.0031, -0.0271],\n",
                            "                      [-0.0414, -0.1474, -0.0933,  ..., -0.1013,  0.0265, -0.0363],\n",
                            "                      ...,\n",
                            "                      [-0.1310, -0.0385, -0.0644,  ...,  0.0333, -0.2073,  0.0157],\n",
                            "                      [-0.0779,  0.0786,  0.0916,  ...,  0.0543, -0.0047, -0.0981],\n",
                            "                      [-0.0426,  0.0082, -0.0152,  ..., -0.2192,  0.2401, -0.1041]])),\n",
                            "             ('FC.2.bias',\n",
                            "              tensor([-3.2184e-02,  6.4490e-01, -1.1673e-01, -6.8304e-02,  8.2250e-01,\n",
                            "                      -1.3536e-01,  2.3962e-01,  6.4875e-01, -2.0511e-01,  4.2482e-01,\n",
                            "                       6.6529e-01, -1.2586e-01, -1.5270e-01, -2.5325e-01,  5.0296e-01,\n",
                            "                       4.2039e-03,  7.2254e-01,  6.9949e-01, -2.2285e-01,  4.0096e-01,\n",
                            "                       2.3508e-03,  7.2486e-01,  3.8453e-01,  3.0223e-01,  1.4659e-01,\n",
                            "                       1.5448e-01, -2.0563e-01,  2.9982e-01, -4.9822e-02,  5.0139e-01,\n",
                            "                       1.8434e-01, -6.6757e-02,  6.2983e-01,  6.2572e-01, -7.9897e-02,\n",
                            "                      -7.8909e-02,  7.1258e-01, -2.4417e-01,  7.4303e-01,  5.5948e-01,\n",
                            "                      -4.5422e-02,  1.0503e-01,  5.2782e-01,  7.5758e-01, -2.3897e-01,\n",
                            "                       8.7017e-01, -3.8250e-01, -1.9519e-01,  7.4551e-02, -8.9068e-02,\n",
                            "                       5.3740e-01, -2.4445e-01, -1.0125e-01, -2.3108e-01, -1.8359e-01,\n",
                            "                       5.6230e-01,  2.9704e-01,  7.7429e-01,  6.3816e-01, -8.1389e-02,\n",
                            "                       7.8359e-01,  7.6466e-01, -1.5846e-01, -9.4436e-02, -1.5405e-01,\n",
                            "                       6.6343e-01, -2.8493e-01,  7.1128e-01,  8.6894e-01,  7.1392e-01,\n",
                            "                       6.1932e-01, -3.5458e-01, -1.6617e-01, -1.2949e-01, -2.3844e-02,\n",
                            "                       7.6666e-02,  7.8438e-01,  7.8596e-01, -1.5958e-01,  4.7224e-01,\n",
                            "                       2.9849e-01, -6.2412e-02, -1.8523e-01, -2.0779e-01,  5.8457e-01,\n",
                            "                      -1.0565e-01,  6.4783e-01,  7.8712e-01, -2.6658e-01, -1.8938e-01,\n",
                            "                       3.7716e-02,  2.2768e-01, -1.4917e-01,  2.4719e-01, -2.0596e-01,\n",
                            "                       5.8987e-01, -1.2992e-01,  7.6591e-01,  7.5481e-01, -1.2976e-01,\n",
                            "                      -2.2780e-01,  2.8045e-01,  8.0394e-01, -2.9520e-02,  2.8060e-02,\n",
                            "                      -4.1769e-02,  5.8684e-01, -7.9207e-02,  6.3732e-01,  6.9381e-01,\n",
                            "                       9.2933e-02,  7.5470e-01,  7.6462e-01, -1.3396e-01, -1.0519e-01,\n",
                            "                       7.2637e-01, -7.3171e-04,  5.4845e-01, -7.9142e-02,  7.2467e-01,\n",
                            "                       4.6807e-01, -8.7934e-02, -2.5029e-01,  7.5226e-01, -1.5999e-01,\n",
                            "                       7.1259e-01,  7.5196e-01, -1.0090e-01])),\n",
                            "             ('FC.3.weight',\n",
                            "              tensor([[-0.0249, -0.1617,  0.0768, -0.1609,  0.0295,  0.1094, -0.0178,  0.0991,\n",
                            "                        0.0277, -0.0514,  0.0761,  0.1390,  0.0269,  0.2035,  0.0688, -0.1727,\n",
                            "                       -0.1347, -0.0266,  0.1629,  0.1161,  0.0432,  0.1406,  0.0495,  0.0455,\n",
                            "                       -0.0612, -0.1292,  0.1099, -0.0608, -0.0038,  0.0885, -0.1182,  0.1397,\n",
                            "                        0.1680, -0.0905,  0.0224, -0.1868,  0.3109, -0.1037,  0.0872,  0.1053,\n",
                            "                        0.0238,  0.1439,  0.0194,  0.1175,  0.0910,  0.0974, -0.1481,  0.0819,\n",
                            "                       -0.0821, -0.0503,  0.0315,  0.0572, -0.2469,  0.0578, -0.0693, -0.0502,\n",
                            "                       -0.1612,  0.1432, -0.0721,  0.0154,  0.0833, -0.1621, -0.0804, -0.1303,\n",
                            "                       -0.0371,  0.2393, -0.0569, -0.0124, -0.0498,  0.0549,  0.1261,  0.1126,\n",
                            "                       -0.0789, -0.1011, -0.0993, -0.0395,  0.0151, -0.1061,  0.0076, -0.2311,\n",
                            "                       -0.0826,  0.2058, -0.1691,  0.1339, -0.1527, -0.0178,  0.0422, -0.0433,\n",
                            "                        0.1817,  0.2777, -0.1162,  0.1776, -0.1962,  0.1585,  0.2206,  0.1102,\n",
                            "                        0.1898, -0.0274,  0.1041,  0.0318,  0.5682,  0.0594,  0.2207, -0.0142,\n",
                            "                       -0.0804, -0.0340, -0.0286,  0.0088,  0.0283,  0.2319,  0.0562, -0.2117,\n",
                            "                        0.1001,  0.1057, -0.0219,  0.1522, -0.1259, -0.0607, -0.0226,  0.1349,\n",
                            "                        0.2102,  0.1157,  0.0826, -0.0583, -0.1881, -0.0976,  0.1627,  0.0469],\n",
                            "                      [ 0.2089, -0.0126,  0.2538, -0.0114, -0.0163, -0.0031, -0.0042, -0.0114,\n",
                            "                       -0.0064, -0.0171, -0.0114,  0.2397,  0.0565,  0.0142, -0.0132,  0.0317,\n",
                            "                       -0.0166, -0.0108,  0.0091, -0.0135,  0.0078, -0.0190, -0.0147, -0.0007,\n",
                            "                        0.0044,  0.0030,  0.1237, -0.0065,  0.0047, -0.0045, -0.0098, -0.0037,\n",
                            "                       -0.0163, -0.0094,  0.0242,  0.0194, -0.0256,  0.1568, -0.0114, -0.0121,\n",
                            "                        0.1253,  0.0162, -0.0148, -0.0109,  0.0117, -0.0118,  0.0091,  0.0178,\n",
                            "                        0.0191,  0.1070, -0.0073, -0.0114, -0.0026,  0.0237,  0.1157, -0.0107,\n",
                            "                       -0.0052, -0.0173, -0.0087,  0.0299, -0.0161, -0.0186,  0.0154,  0.0101,\n",
                            "                       -0.0006, -0.0156,  0.0362, -0.0040, -0.0204, -0.0121, -0.0091,  0.0021,\n",
                            "                       -0.0164,  0.1656, -0.0009, -0.0055, -0.0084, -0.0101,  0.1153, -0.0139,\n",
                            "                       -0.0125, -0.0433,  0.0277, -0.0131, -0.0088,  0.0059, -0.0128, -0.0123,\n",
                            "                       -0.0020,  0.0261,  0.0154, -0.0079,  0.0082, -0.0061,  0.1023, -0.0078,\n",
                            "                       -0.0099, -0.0205, -0.0095, -0.0228,  0.0822, -0.0022, -0.0163,  0.0237,\n",
                            "                        0.0131, -0.0155, -0.0084, -0.0055, -0.0136, -0.0089, -0.0070, -0.0118,\n",
                            "                       -0.0220,  0.0079,  0.0348, -0.0076, -0.0070, -0.0115,  0.0274, -0.0077,\n",
                            "                       -0.0140,  0.0124,  0.0282, -0.0138,  0.0180, -0.0163, -0.0142,  0.0057]])),\n",
                            "             ('FC.3.bias', tensor([-0.0803, -0.9165]))])"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_dict"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2021-10-16 12:51:14\n"
                    ]
                }
            ],
            "source": [
                "import pickle\n",
                "import optuna\n",
                "from optuna.trial import TrialState\n",
                "from gnn import gnn\n",
                "import time\n",
                "import numpy as np\n",
                "import utils\n",
                "from utils import *\n",
                "import torch.nn as nn\n",
                "import torch\n",
                "import time\n",
                "import os\n",
                "from collections import defaultdict\n",
                "import argparse\n",
                "import time\n",
                "from torch.utils.data import DataLoader                                     \n",
                "from graphformer_dataset import graphformerDataset, collate_fn, DTISampler\n",
                "now = time.localtime()\n",
                "s = \"%04d-%02d-%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
                "print (s)\n",
                "# os.chdir(os.path.abspath(os.path.dirname(__file__)))\n",
                "# print(os.path.abspath(os.path.dirname(__file__)))\n",
                "# print(os.getcwd())\n",
                "parser = argparse.ArgumentParser()\n",
                "parser.add_argument(\"--lr\", help=\"learning rate\", type=float, default = 0.0001)\n",
                "parser.add_argument(\"--epoch\", help=\"epoch\", type=int, default = 1000)\n",
                "parser.add_argument(\"--ngpu\", help=\"number of gpu\", type=int, default = 1)\n",
                "parser.add_argument(\"--batch_size\", help=\"batch_size\", type=int, default = 24)\n",
                "parser.add_argument(\"--num_workers\", help=\"number of workers\", type=int, default = 8)\n",
                "parser.add_argument(\"--n_graph_layer\", help=\"number of GNN layer\", type=int, default = 2)\n",
                "# parser.add_argument(\"--d_graph_layer\", help=\"dimension of GNN layer\", type=int, default = 140)\n",
                "parser.add_argument(\"--n_FC_layer\", help=\"number of FC layer\", type=int, default = 4)\n",
                "parser.add_argument(\"--d_FC_layer\", help=\"dimension of FC layer\", type=int, default = 128)\n",
                "parser.add_argument(\"--data_path\", help=\"file path of dude data\", type=str, default='/home/caoduanhua/score_function/data/pocket_data')\n",
                "#/home/jiangjiaxin/../../../\n",
                "parser.add_argument(\"--save_dir\", help=\"save directory of model parameter\", type=str, default ='../train_result/optuna/')\n",
                "parser.add_argument(\"--initial_mu\", help=\"initial value of mu\", type=float, default = 4.0)\n",
                "parser.add_argument(\"--initial_dev\", help=\"initial value of dev\", type=float, default = 1.0)\n",
                "parser.add_argument(\"--dropout_rate\", help=\"dropout_rate\", type=float, default = 0.2)\n",
                "#args.attention_dropout_rate\n",
                "parser.add_argument(\"--attention_dropout_rate\", help=\"attention_dropout_rate\", type=float, default = 0.2)\n",
                "parser.add_argument(\"--train_keys\", help=\"train keys\", type=str, default='./keys/train_keys.pkl')\n",
                "parser.add_argument(\"--test_keys\", help=\"test keys\", type=str, default='./keys/test_keys.pkl')\n",
                "#add by caooduanhua\n",
                "# self.fundation_model = args.fundation_model\n",
                "parser.add_argument(\"--fundation_model\", help=\"what kind of model to use : paper or graphformer\", type=str, default='paper')\n",
                "parser.add_argument(\"--layer_type\", help=\"what kind of layer to use :GAT_gata,MH_gate,transformer_gate,graphformer\", type=str, default='GAT_gate')\n",
                "parser.add_argument(\"--loss_fn\", help=\"what kind of loss_fn to use : bce_loss facal_loss \", type=str, default='bce_loss')\n",
                "# args.gate\n",
                "parser.add_argument(\"--only_adj2\", help=\"adj2 only have 0 1 \", action = 'store_true')\n",
                "parser.add_argument(\"--only_dis_adj2\", help=\"sdj2 only have distance info \", action = 'store_true')\n",
                "parser.add_argument(\"--share_layer\", help=\"select share layers with h1 h2 or not \", action = 'store_false')\n",
                "parser.add_argument(\"--use_adj\", help=\"select sampler in train stage \", action = 'store_false')\n",
                "parser.add_argument(\"--mode\", help=\"what kind of mode to training : only h1 to training or h1 h2 to training [1_H,2_H] \", type=str, default='1_H')\n",
                "parser.add_argument(\"--n_in_feature\", help=\"dim before layers to tranform dim in paper model\", type=int, default = 80)\n",
                "parser.add_argument(\"--n_out_feature\", help=\"dim in layers\", type=int, default = 80)\n",
                "parser.add_argument(\"--ffn_size\", help=\"ffn dim in transformer type layers\", type=int, default = 280)\n",
                "parser.add_argument(\"--head_size\", help=\"multihead attention\", type=int, default = 8)\n",
                "parser.add_argument(\"--patience\", help=\"patience for early stop\", type=int, default = 1000)\n",
                "parser.add_argument(\"--gate\", help=\"gate mode for Transformer_gate\", action = 'store_true')\n",
                "parser.add_argument(\"--debug\", help=\"debug mode for check\", action = 'store_true')\n",
                "parser.add_argument(\"--test\", help=\"independent tests or not \", action = 'store_true')\n",
                "parser.add_argument(\"--sampler\", help=\"select sampler in train stage \", action = 'store_true')\n",
                "parser.add_argument(\"--A2_limit\", help=\"select add a A2adj strong limit  in model\", action = 'store_true')\n",
                "parser.add_argument(\"--test_path\", help=\"test keys\", type=str, default='/home/duanhua/data/pocket_sample_70w/train')\n",
                "parser.add_argument(\"--path_data_dir\", help=\"saved shortest path data\", type=str, default='../../data/pocket_data_path')\n",
                "parser.add_argument(\"--EF_rates\", help=\"eval EF value in different percentage\",nargs='+', type=float, default = 0.01)\n",
                "#parser.add_argument('--nargs-int-type', nargs='+', type=int)\n",
                "parser.add_argument(\"--multi_hop_max_dist\", help=\"how many edges to use in multi-hop edge bias\", type=int, default = 10)\n",
                "parser.add_argument(\"--edge_type\", help=\"use multi-hop edge or not:single or multi_hop \", type=str, default='single')\n",
                "parser.add_argument(\"--rel_pos_bias\", help=\"add rel_pos_bias or not default not \", action = 'store_true') \n",
                "parser.add_argument(\"--edge_bias\", help=\"add edge_bias or not default not \", action = 'store_true')       \n",
                "parser.add_argument(\"--rel_3d_pos_bias\", help=\"add rel_3d_pos_bias or not default not \", action = 'store_true')        \n",
                "parser.add_argument(\"--in_degree_bias\", help=\"add in_degree_bias or not default not \", action = 'store_true')  \n",
                "parser.add_argument(\"--out_degree_bias\", help=\"add out_degree_bias or not default not \", action = 'store_true')          \n",
                "# save_model\n",
                "parser.add_argument(\"--hot_start\", help=\"hot start\", action = 'store_true')\n",
                "parser.add_argument(\"--save_model\", help=\"hot start\", type=str, default='/home/caoduanhua/score_function/GNN/train_result/optuna/paper/GAT_gate/2021-10-14-14-24-37/save_best_model.pt')\n",
                "parser.add_argument(\"--lr_decay\", help=\"use lr decay \", action = 'store_true')  \n",
                "# auxiliary_loss\n",
                "parser.add_argument(\"--auxiliary_loss\", help=\"use lr decay \", action = 'store_true') \n",
                "parser.add_argument(\"--r_drop\", help=\"use lr decay \", action = 'store_true') \n",
                "parser.add_argument(\"--deta_const\", help=\"const deta \", action = 'store_true') \n",
                "parser.add_argument(\"--alpha\", help=\"use lr decay \", type = int,default = 5) \n",
                "#这套参数默认是paper+ GAT——gate without attn_bias\n",
                "args = parser.parse_args([])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<All keys matched successfully>"
                        ]
                    },
                    "execution_count": 75,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model = gnn(args)\n",
                "model.load_state_dict(model_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [],
            "source": [
                "# for param in model.parameters():\n",
                "#     # if param == 'FC':\n",
                "#         print(param)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [],
            "source": [
                "# torch.nn.init.xavier_normal_(model.FC[0].weight)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [],
            "source": [
                "# model.FC[0].weight"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "dict_keys(['mu', 'dev', 'embede.weight', 'gconv1.0.A', 'gconv1.0.W.weight', 'gconv1.0.W.bias', 'gconv1.0.gate.weight', 'gconv1.0.gate.bias', 'gconv1.1.A', 'gconv1.1.W.weight', 'gconv1.1.W.bias', 'gconv1.1.gate.weight', 'gconv1.1.gate.bias'])"
                        ]
                    },
                    "execution_count": 79,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_dict = {k:v for k,v in model_dict.items() if 'FC' not in k}\n",
                "model_dict.keys()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<All keys matched successfully>"
                        ]
                    },
                    "execution_count": 82,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# model.state_dict().update(model_dict)\n",
                "state_dict = model.state_dict()\n",
                "state_dict.update(model_dict)\n",
                "model.load_state_dict(state_dict)\n",
                "# state_dict"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Parameter containing:\n",
                        "tensor([3.7809], requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([1.2101], requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([[ 0.0363,  0.0743, -0.0217,  ..., -0.2698, -0.3135,  0.1028],\n",
                        "        [-0.0408, -0.0882, -0.0775,  ..., -0.0206,  0.0575,  0.2330],\n",
                        "        [ 0.0518,  0.0273,  0.1567,  ..., -0.0966, -0.1696, -0.2364],\n",
                        "        ...,\n",
                        "        [ 0.1760, -0.0919,  0.1199,  ..., -0.1398, -0.0163, -0.2423],\n",
                        "        [-0.0449,  0.0531,  0.0568,  ...,  0.0466,  0.0087, -0.0377],\n",
                        "        [-0.1153,  0.1746, -0.0156,  ..., -0.0655, -0.0559, -0.1215]],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([[-0.0985,  0.2204, -0.0274,  ..., -0.0688,  0.0439, -0.0928],\n",
                        "        [-0.0391,  0.1893, -0.0824,  ...,  0.0879, -0.0622, -0.0465],\n",
                        "        [-0.2448, -0.1199, -0.0481,  ..., -0.0073,  0.1488,  0.0917],\n",
                        "        ...,\n",
                        "        [ 0.1219, -0.0438, -0.3918,  ..., -0.0072, -0.0887, -0.0224],\n",
                        "        [-0.0361, -0.0682, -0.1530,  ..., -0.0914, -0.0638,  0.0097],\n",
                        "        [-0.2063,  0.1800,  0.2504,  ..., -0.1119,  0.1271,  0.2087]],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([[ 0.0870,  0.1629, -0.0114,  ..., -0.0618, -0.1096,  0.1113],\n",
                        "        [-0.0959,  0.0528,  0.1104,  ..., -0.1888, -0.1989,  0.1985],\n",
                        "        [-0.0156, -0.0523, -0.0581,  ...,  0.1428,  0.0475, -0.0499],\n",
                        "        ...,\n",
                        "        [-0.0878,  0.0575,  0.3532,  ...,  0.1690, -0.1206, -0.0503],\n",
                        "        [-0.0464, -0.0051,  0.0768,  ..., -0.1789, -0.0342, -0.0410],\n",
                        "        [-0.0068, -0.0799,  0.0257,  ...,  0.0150,  0.0299, -0.0632]],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([-0.0827, -0.1749, -0.0941, -0.2086, -0.1608, -0.0095,  0.0490, -0.0556,\n",
                        "        -0.1341, -0.1544, -0.0270,  0.0147,  0.0907, -0.0974, -0.0504, -0.0907,\n",
                        "         0.0308,  0.1247, -0.1072,  0.0288, -0.1152, -0.1633, -0.0817, -0.0721,\n",
                        "         0.0629, -0.0857,  0.0358,  0.0460, -0.1135, -0.1171,  0.0992, -0.0212,\n",
                        "        -0.0516, -0.2298, -0.0033, -0.0202, -0.0483, -0.0934, -0.1794, -0.0855,\n",
                        "         0.0423, -0.0725,  0.0592, -0.0729, -0.0421, -0.0938, -0.1722,  0.0443,\n",
                        "         0.0680, -0.1140, -0.1320, -0.0514, -0.0555,  0.0289, -0.1399, -0.0463,\n",
                        "        -0.3020, -0.0802, -0.1149, -0.1248, -0.0281, -0.1001,  0.0178, -0.0183,\n",
                        "        -0.1306, -0.0812, -0.2715, -0.0117, -0.1469,  0.0016, -0.2374,  0.1193,\n",
                        "        -0.1479,  0.0579, -0.1749, -0.0483, -0.0182, -0.1350,  0.0212, -0.1256],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([[ 0.0769,  0.0358, -0.0338, -0.1222, -0.0847, -0.0710, -0.0294,  0.0739,\n",
                        "          0.1475, -0.0166, -0.0577,  0.0101,  0.0640, -0.0784, -0.0218,  0.0112,\n",
                        "          0.1170,  0.0354, -0.0693, -0.0765, -0.1495, -0.1078, -0.0404,  0.1499,\n",
                        "         -0.0075, -0.0484, -0.0479, -0.1632, -0.0115, -0.0659,  0.1097,  0.0075,\n",
                        "          0.0789,  0.0263, -0.0202, -0.0400,  0.0492, -0.1074, -0.0087, -0.0480,\n",
                        "          0.1362, -0.0874,  0.0281, -0.0313, -0.1566,  0.0246,  0.0214,  0.1733,\n",
                        "         -0.1230, -0.0352, -0.0917,  0.1523,  0.0138, -0.0173, -0.0516, -0.1060,\n",
                        "         -0.0494,  0.0631, -0.1066,  0.1823,  0.0693, -0.0210, -0.0007,  0.0349,\n",
                        "          0.0017,  0.0748, -0.0302, -0.1384,  0.1350,  0.0177, -0.1025, -0.0572,\n",
                        "          0.1345, -0.0712,  0.0158, -0.0213,  0.1741,  0.1619, -0.0477,  0.0148,\n",
                        "          0.0367,  0.1993,  0.1095,  0.4478,  0.0759,  0.1757,  0.0271, -0.0953,\n",
                        "          0.1466,  0.2111, -0.0095, -0.1127,  0.1178,  0.5421,  0.0853,  0.0868,\n",
                        "          0.1013,  0.0543,  0.0410,  0.0992,  0.0335,  0.3109,  0.1637,  0.0150,\n",
                        "         -0.0393, -0.0665,  0.0689,  0.1354,  0.3198, -0.0643,  0.0196,  0.0560,\n",
                        "          0.0823,  0.2879,  0.1035, -0.0845, -0.1035,  0.1947,  0.3383,  0.1526,\n",
                        "         -0.0969,  0.2362, -0.1464,  0.3579, -0.1005, -0.2078,  0.3106,  0.0669,\n",
                        "          0.0519,  0.1018, -0.1242,  0.0769,  0.1667,  0.0585, -0.0509,  0.0925,\n",
                        "          0.3614,  0.0908, -0.0425,  0.1067,  0.1372, -0.1159,  0.0555,  0.1039,\n",
                        "         -0.0175, -0.0387,  0.0166,  0.1026, -0.0272, -0.2708,  0.1751,  0.0956,\n",
                        "          0.1660,  0.0315,  0.0818,  0.0444,  0.1605,  0.3855,  0.0478,  0.0161]],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([0.1842], requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([[ 0.1851,  0.0372, -0.0684,  ..., -0.0160, -0.1679, -0.2538],\n",
                        "        [ 0.0377, -0.0173,  0.3291,  ...,  0.0991,  0.0652, -0.0147],\n",
                        "        [ 0.1202,  0.3177,  0.0279,  ...,  0.0163, -0.2043,  0.1024],\n",
                        "        ...,\n",
                        "        [-0.1633,  0.1582, -0.2106,  ..., -0.0767,  0.0097,  0.0021],\n",
                        "        [ 0.0123, -0.1141, -0.1556,  ..., -0.0574, -0.0014,  0.1070],\n",
                        "        [-0.1265,  0.1019,  0.2868,  ...,  0.1354,  0.3012,  0.1705]],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([[ 0.1647,  0.0359,  0.2450,  ...,  0.2769, -0.0732,  0.1834],\n",
                        "        [ 0.1421, -0.2015,  0.2013,  ...,  0.2970, -0.3325,  0.0170],\n",
                        "        [-0.5341,  0.6693, -0.1920,  ..., -0.0121, -0.0532,  0.7529],\n",
                        "        ...,\n",
                        "        [-0.2130, -0.3370,  0.0960,  ..., -0.1729,  0.0491, -0.0212],\n",
                        "        [-0.1958, -0.0532, -0.1005,  ..., -0.0870,  0.0609, -0.0487],\n",
                        "        [-0.4055, -0.1650, -0.0099,  ..., -0.2497,  0.0465, -0.0870]],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([-0.0473,  0.0069, -0.0266,  0.0411, -0.1430,  0.0422,  0.0510, -0.0446,\n",
                        "         0.0481, -0.0732, -0.0211,  0.0217,  0.0253, -0.0530,  0.0316, -0.1244,\n",
                        "        -0.0471,  0.0159, -0.0537, -0.0505,  0.0141, -0.0144, -0.1859, -0.1317,\n",
                        "        -0.2624,  0.0019,  0.0033, -0.0327,  0.0294,  0.0463,  0.0413, -0.0430,\n",
                        "         0.0129, -0.1121, -0.2155, -0.0263,  0.0239,  0.0569, -0.1668,  0.0194,\n",
                        "        -0.0601, -0.1533, -0.0553, -0.0456, -0.0971,  0.0198, -0.0279,  0.0091,\n",
                        "        -0.0749, -0.0649, -0.1334, -0.0413, -0.3710, -0.0574, -0.3876,  0.0238,\n",
                        "         0.0082, -0.0262, -0.0111, -0.1537, -0.0798,  0.0011, -0.0107,  0.0048,\n",
                        "        -0.0719, -0.0495, -0.1278, -0.0218, -0.0160, -0.1406,  0.0063, -0.0185,\n",
                        "        -0.1262, -0.1645, -0.0591,  0.0206, -0.0288, -0.0176, -0.0712,  0.0043],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([[ 0.1810, -0.3557, -0.2245, -0.0427,  0.5756,  0.1169, -0.1632,  0.2606,\n",
                        "         -0.1449, -0.0619,  0.1705, -0.1275, -0.2367, -0.1304,  0.0795,  0.1902,\n",
                        "         -0.1586, -0.0414,  0.1230,  0.0124,  0.1433,  0.0085,  0.2473,  0.1631,\n",
                        "         -0.1218, -0.0742,  0.1360, -0.2616, -0.2909, -0.2445, -0.1207, -0.0475,\n",
                        "          0.0169, -0.1053,  0.1975, -0.0273, -0.1573, -0.4062, -0.2661, -0.1031,\n",
                        "          0.0220,  0.2851, -0.1073,  0.1355, -0.1550,  0.0724, -0.1380,  0.2960,\n",
                        "         -0.0327, -0.0265, -0.3225,  0.2711,  0.3116,  0.4147, -0.8220, -0.2197,\n",
                        "          0.4298,  0.2265,  0.1264, -0.0527, -0.0110, -0.1322,  0.1252,  0.2073,\n",
                        "         -0.0240,  0.1138, -0.0172,  0.1382, -0.0904, -0.1861,  0.1365,  0.0312,\n",
                        "         -0.2688,  0.4498, -0.0601, -0.2220,  0.1052, -0.0932,  0.0345, -0.0667,\n",
                        "          0.5951,  0.6232,  0.5190,  0.0929, -0.4014,  0.1623,  0.4780, -0.1579,\n",
                        "          0.2860, -0.5402, -0.1133,  0.4171,  0.1635,  0.2705,  0.3115,  0.3635,\n",
                        "         -0.2060,  0.1479,  0.6442, -0.1534,  0.1338,  0.1084, -0.2822,  0.3096,\n",
                        "         -0.4876,  0.3451,  0.2520,  0.2325,  0.2535,  0.0018,  0.2805,  0.7534,\n",
                        "          0.3015, -0.2934, -0.0048,  0.0112,  0.3410, -0.3485, -0.6962,  0.2207,\n",
                        "         -0.0982, -0.4035,  0.5509, -0.1425, -0.1402,  0.2941,  0.6331,  0.3353,\n",
                        "          0.5676,  0.0691, -0.4238,  0.1657, -0.4559,  0.6863, -0.3392,  0.3613,\n",
                        "          0.1877, -0.1053,  0.1808, -0.1722,  0.2103, -0.1799,  0.4185,  0.1307,\n",
                        "          0.1620,  0.5922, -0.3707,  0.5979,  0.1003, -0.4746,  0.1216, -0.2777,\n",
                        "         -0.6022, -0.3167,  0.1361, -0.0156, -0.0460,  0.7135,  0.0579, -0.0019]],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([-0.0541], requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([[ 0.4098, -0.3213,  0.0157,  ...,  0.2204, -0.3438, -0.2356],\n",
                        "        [-0.6136, -0.1221, -0.0353,  ...,  0.0166, -0.4735,  0.2328],\n",
                        "        [ 0.4139,  0.3662, -0.1438,  ..., -0.1724,  0.0947,  0.1353],\n",
                        "        ...,\n",
                        "        [-0.4121,  0.0547,  0.3274,  ...,  0.1114,  0.5682,  0.0874],\n",
                        "        [-0.1738, -0.0728,  0.0607,  ..., -0.0859,  0.1798,  0.0727],\n",
                        "        [-0.5863,  0.0099, -0.1748,  ..., -0.0267, -0.1310, -0.0961]],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([-0.0981, -0.0678, -0.2452, -0.2420, -0.1813, -0.2874,  0.2398, -0.0470,\n",
                        "         0.0767, -0.0089,  0.0239,  0.0372,  0.2578,  0.1033, -0.3008, -0.0057,\n",
                        "        -0.0732, -0.1009, -0.0877, -0.1371, -0.0141,  0.0516, -0.2676, -0.4275,\n",
                        "        -0.2869,  0.2278, -0.3679, -0.0144, -0.3139, -0.3432,  0.1102, -0.2814,\n",
                        "         0.0759, -0.2239, -0.1025, -0.1589, -0.1912, -0.1917, -0.0639, -0.0439,\n",
                        "        -0.2942, -0.0534, -0.1303, -0.3118,  0.0766, -0.0589,  0.0251, -0.0195,\n",
                        "         0.2307, -0.2093, -0.1965,  0.0472, -0.1216, -0.1547, -0.2175, -0.0703,\n",
                        "        -0.3034, -0.0879, -0.2929,  0.0864,  0.0207, -0.0083, -0.1491,  0.1393,\n",
                        "         0.0141, -0.2931, -0.1364,  0.0564,  0.0724, -0.0717, -0.3009, -0.0755,\n",
                        "        -0.0704, -0.2671, -0.2421, -0.1536,  0.1151, -0.0846, -0.1393, -0.0305,\n",
                        "        -0.2726,  0.2106, -0.1498, -0.1157, -0.0861, -0.2366,  0.0765, -0.2977,\n",
                        "        -0.4042,  0.0959, -0.1967, -0.0375, -0.2929, -0.1330,  0.1634, -0.0671,\n",
                        "         0.0107, -0.2378, -0.2305, -0.3989, -0.1172, -0.1238, -0.1112, -0.0394,\n",
                        "        -0.1105,  0.1340, -0.2163, -0.2339, -0.3172, -0.0634, -0.3542, -0.3278,\n",
                        "         0.2197,  0.1445, -0.1468, -0.3023,  0.0294, -0.0079, -0.2355, -0.0917,\n",
                        "         0.0047,  0.1674,  0.0015, -0.1293, -0.2808,  0.0947, -0.0249, -0.1279],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([[-0.7621,  0.0639, -0.2740,  ...,  0.1893, -0.0142,  0.0692],\n",
                        "        [ 0.1179,  0.1644, -0.1033,  ...,  0.2558,  0.0796,  0.0361],\n",
                        "        [-0.1190, -0.3500, -0.2911,  ..., -0.0747, -0.2019, -0.0808],\n",
                        "        ...,\n",
                        "        [-0.2399,  0.0115, -0.2256,  ..., -0.6322, -0.0247, -0.5789],\n",
                        "        [-0.3765, -0.0886,  0.1646,  ..., -0.4966, -0.7434, -0.3793],\n",
                        "        [ 0.2048, -0.1055, -0.4546,  ...,  0.0339, -0.5289,  0.1802]],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([-0.1709, -0.1109, -0.2321, -0.1016,  0.0489, -0.1053,  0.0031,  0.0091,\n",
                        "         0.0890,  0.0624,  0.1068, -0.1975,  0.0655, -0.1248, -0.0536, -0.0415,\n",
                        "         0.0751,  0.0925, -0.2610, -0.0730,  0.0006,  0.0023,  0.0130, -0.0325,\n",
                        "        -0.0979, -0.0659, -0.0537, -0.1340, -0.0506, -0.0983, -0.0025,  0.0907,\n",
                        "        -0.0179,  0.0803, -0.1019, -0.1482, -0.0010,  0.0388, -0.0371,  0.0461,\n",
                        "         0.0036,  0.0589, -0.0208,  0.1140, -0.1009, -0.0007, -0.0203,  0.0137,\n",
                        "        -0.3696, -0.0027, -0.0521,  0.0057, -0.0604, -0.0091, -0.0315,  0.0268,\n",
                        "        -0.0095, -0.0075, -0.1551, -0.0072, -0.0131, -0.0780,  0.0124, -0.0916,\n",
                        "        -0.0147, -0.0339, -0.0610,  0.0079,  0.0547, -0.0503, -0.0970,  0.0314,\n",
                        "        -0.0080,  0.0091,  0.0308, -0.0474, -0.0899, -0.0813,  0.0657, -0.0010,\n",
                        "         0.0171, -0.0104, -0.0865,  0.0367, -0.0147,  0.0132, -0.0253, -0.1497,\n",
                        "        -0.0965,  0.0347,  0.0629, -0.1116, -0.0312, -0.0370,  0.0017, -0.0009,\n",
                        "        -0.0785, -0.2514,  0.0764,  0.0391, -0.1976, -0.0272, -0.0019,  0.0314,\n",
                        "        -0.0180,  0.0069, -0.0215, -0.2446, -0.1026, -0.0265, -0.0442, -0.1802,\n",
                        "        -0.1523,  0.0214, -0.0144,  0.0516, -0.0149, -0.2399,  0.0803, -0.2165,\n",
                        "        -0.1119,  0.0126, -0.0419, -0.1744, -0.1348,  0.0124, -0.0354, -0.1416],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([[ 0.0176,  0.4066,  0.3179,  ..., -0.4719, -0.4289, -0.0636],\n",
                        "        [-0.3963, -0.2082, -0.2048,  ...,  0.1589,  0.0781, -0.0443],\n",
                        "        [ 0.1900,  0.2754,  0.0939,  ..., -0.1092, -0.1036,  0.4869],\n",
                        "        ...,\n",
                        "        [-0.3515, -0.2900, -0.0201,  ..., -0.0106,  0.1500, -0.1117],\n",
                        "        [-0.2395,  0.5879,  0.2916,  ..., -0.2020, -0.2080, -0.1326],\n",
                        "        [-0.1528, -0.2698,  0.1517,  ..., -0.0658,  0.0035, -0.3053]],\n",
                        "       requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([ 1.4369e-01,  1.6702e-01,  1.5346e-01,  9.4738e-02,  1.6489e-01,\n",
                        "         1.7919e-01,  2.3880e-01, -3.0332e-02,  1.7040e-01,  5.6409e-02,\n",
                        "         1.3039e-01,  1.2625e-01,  1.8365e-01,  1.7661e-01,  1.9890e-01,\n",
                        "         9.6493e-02,  7.0696e-02, -1.7642e-01, -3.1893e-01,  2.0068e-01,\n",
                        "         8.0659e-02,  2.0001e-01,  1.4680e-01,  1.9674e-01, -7.7459e-06,\n",
                        "         1.5805e-01,  1.1348e-01,  6.6840e-02, -1.7683e-01, -2.8215e-01,\n",
                        "         1.5843e-01,  4.0570e-02,  1.3669e-01,  5.3543e-02,  1.3198e-01,\n",
                        "         1.5967e-01,  2.0321e-01,  2.2376e-01,  1.9149e-01,  1.2692e-01,\n",
                        "         9.8828e-02,  1.6661e-01,  1.5322e-01,  1.2408e-01,  1.0400e-01,\n",
                        "         1.2925e-01,  1.8488e-01,  1.1093e-01,  1.9543e-01,  1.8401e-01,\n",
                        "         1.6457e-01,  1.6513e-01,  9.1305e-02,  1.4671e-01,  1.7466e-01,\n",
                        "         1.4522e-01,  8.5766e-02,  2.0398e-01,  1.2597e-01,  7.4274e-02,\n",
                        "        -3.0016e-01,  9.7954e-02,  1.2104e-01,  9.5635e-02,  1.2264e-01,\n",
                        "         5.1439e-02,  2.0745e-01,  2.9995e-02,  1.6149e-01,  1.6886e-01,\n",
                        "        -2.3950e-01,  1.8016e-01,  2.0069e-01,  1.6890e-01,  1.8967e-01,\n",
                        "         1.1453e-01,  1.6056e-01, -1.5343e-01,  1.6405e-01, -1.0205e-01,\n",
                        "         1.6564e-01, -1.0503e-01,  1.2988e-01,  1.6892e-01,  1.5005e-01,\n",
                        "         1.2412e-01,  1.4719e-01,  1.2697e-01,  1.7564e-01, -1.4990e-01,\n",
                        "         1.8184e-01,  1.2880e-01,  1.4065e-01,  1.4855e-01,  1.7648e-01,\n",
                        "         1.4561e-01,  1.0548e-01,  1.7155e-01,  1.1118e-01,  1.8814e-01,\n",
                        "         7.8228e-02,  1.4622e-01,  1.1502e-01, -7.3830e-02,  1.2499e-01,\n",
                        "         1.5405e-01,  2.4385e-01,  1.7127e-01,  1.4532e-01, -5.7688e-02,\n",
                        "         8.2367e-02, -1.6149e-01,  1.1263e-01, -3.2462e-01,  1.5449e-01,\n",
                        "        -1.2651e-01,  1.7157e-01,  1.2085e-01,  4.6255e-02,  1.2306e-01,\n",
                        "         1.0118e-01,  1.6173e-01,  1.5823e-01, -2.6658e-01,  5.9389e-02,\n",
                        "         1.4064e-01,  1.9317e-01,  5.0767e-02], requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([[-1.6881e-01,  7.7479e-02, -1.4099e-01,  8.9026e-02, -1.0501e-01,\n",
                        "          7.4622e-02,  1.4733e-01,  1.7688e-01,  9.9709e-02,  2.2913e-01,\n",
                        "          3.4105e-04,  9.5131e-02, -1.1938e-02,  1.6694e-01, -9.4221e-02,\n",
                        "         -8.1255e-02, -1.9142e-01,  1.0069e-01, -4.4806e-02,  1.1176e-01,\n",
                        "          2.6567e-02, -5.5315e-02,  1.5858e-01, -8.7894e-02, -1.2284e-01,\n",
                        "          5.5285e-02, -1.3539e-02, -2.9872e-01, -1.0374e-01,  1.1978e-01,\n",
                        "         -2.1098e-01, -6.0084e-02, -1.1885e-01,  7.6920e-03,  2.3849e-03,\n",
                        "          3.1851e-02,  1.8154e-01, -2.1697e-01,  5.8482e-02,  2.7915e-01,\n",
                        "          1.4695e-01,  1.2522e-01,  4.1777e-03,  1.8000e-01,  1.7756e-01,\n",
                        "         -2.3600e-01, -9.2294e-02,  6.1183e-02,  1.4871e-01, -5.8600e-02,\n",
                        "          1.3205e-01, -8.1831e-02,  2.5246e-01, -2.8518e-01, -5.9077e-02,\n",
                        "         -8.0182e-02,  5.5872e-02, -6.8623e-02, -1.3160e-01,  3.3735e-02,\n",
                        "         -1.2156e-01,  1.8650e-02,  2.3045e-02,  9.4069e-02,  9.7007e-02,\n",
                        "         -1.0633e-01,  6.4204e-02, -1.7059e-01,  2.8797e-02, -2.3437e-02,\n",
                        "          1.1380e-02,  4.8746e-02,  1.1224e-02,  7.8993e-02,  2.2762e-02,\n",
                        "         -6.1053e-02,  7.8996e-02, -1.0270e-01,  1.0627e-01,  1.5994e-01,\n",
                        "         -3.5769e-02,  9.8159e-02,  6.7603e-02,  2.3380e-01, -7.4368e-03,\n",
                        "         -1.6647e-01, -1.9242e-01, -7.5104e-02, -4.3603e-02, -5.2879e-02,\n",
                        "          1.6348e-01,  3.0641e-02, -2.0625e-01,  4.7818e-02, -2.2568e-01,\n",
                        "         -1.8584e-01, -1.0079e-01,  6.9481e-02, -2.0697e-01,  1.3762e-01,\n",
                        "          9.0204e-02, -8.9806e-02, -1.0481e-01,  1.7019e-02, -1.6650e-01,\n",
                        "          7.0858e-02, -3.9854e-02,  1.4983e-01,  1.1916e-01,  7.6771e-02,\n",
                        "          6.9610e-02,  6.3607e-02, -6.1486e-02, -6.1202e-02, -1.3057e-01,\n",
                        "         -7.7226e-02, -7.6375e-02,  1.3386e-02, -5.0934e-02,  3.1912e-02,\n",
                        "         -5.2110e-02,  1.8207e-01,  5.0415e-02, -1.0638e-02,  2.3211e-01,\n",
                        "          1.8940e-02, -1.2618e-01, -9.1863e-02],\n",
                        "        [-7.8435e-01,  2.1767e-01, -2.9739e-01, -6.1122e-01,  1.5762e-01,\n",
                        "          7.2717e-01,  2.8636e-01, -8.9034e-01,  2.5314e-01,  2.1586e-01,\n",
                        "         -9.7763e-01, -7.3246e-01, -6.1603e-01, -3.7718e-01,  3.0391e-01,\n",
                        "          2.6484e-01,  9.2484e-01, -7.5865e-03, -9.1680e-01, -4.0857e-01,\n",
                        "          1.5421e-01,  2.0613e-01, -7.8937e-01,  5.9515e-01, -3.8541e-01,\n",
                        "         -4.6187e-01, -7.1068e-01, -7.1792e-01,  7.2000e-01, -6.9708e-01,\n",
                        "         -1.1873e+00,  1.6144e-01, -5.0167e-01, -8.2701e-01,  3.0401e-01,\n",
                        "         -3.5506e-01,  6.4779e-01,  1.9059e-01, -3.9428e-01,  1.9999e-01,\n",
                        "         -7.7418e-01, -3.5507e-01,  2.7949e-01,  2.2140e-01, -4.2640e-01,\n",
                        "         -7.2029e-01, -3.8820e-01,  7.1111e-01, -4.7932e-01, -3.8313e-01,\n",
                        "          2.2737e-01, -5.2581e-01,  2.0087e-01, -7.0443e-01, -3.1724e-01,\n",
                        "          1.9359e-01,  1.5527e-01,  2.3994e-01, -5.2640e-01,  2.7064e-01,\n",
                        "         -7.6145e-01, -7.5475e-01,  1.8018e-01,  4.2032e-01, -5.2598e-01,\n",
                        "          1.1942e+00, -3.8135e-01, -6.4185e-01,  1.9153e-01, -3.0841e-01,\n",
                        "         -7.3697e-02,  2.3084e-01,  1.9547e-01, -3.9271e-01, -3.8671e-01,\n",
                        "         -8.9230e-01,  2.6458e-01, -8.6430e-01, -4.3125e-01, -8.6889e-01,\n",
                        "         -3.7672e-01,  4.8934e-01, -6.2822e-01,  2.3689e-01,  5.0755e-01,\n",
                        "          1.6334e-01, -6.8397e-01, -4.8402e-01, -3.6364e-01, -1.9724e-02,\n",
                        "         -3.4938e-01,  1.6654e-01, -4.2221e-01, -5.6964e-01,  2.0965e-01,\n",
                        "          2.7617e-01,  2.1832e-01,  3.9586e-01, -3.7477e-01,  5.3058e-01,\n",
                        "          2.3868e-01,  2.2245e-01,  1.2323e+00,  9.2185e-01, -6.8331e-01,\n",
                        "         -6.2227e-01,  2.3900e-01,  2.5265e-01,  2.2366e-01,  8.8927e-01,\n",
                        "         -6.7229e-01,  9.5096e-01, -8.9285e-01, -6.9210e-01, -1.1799e+00,\n",
                        "          8.3159e-01, -7.4414e-01, -8.9401e-01,  3.6017e-01, -5.7484e-01,\n",
                        "          2.5624e-01,  1.9564e-01, -4.3319e-01,  1.0139e+00,  2.1148e-01,\n",
                        "          2.6647e-01, -3.4005e-01,  5.7931e-01]], requires_grad=True)\n",
                        "Parameter containing:\n",
                        "tensor([0.0659, 0.2815], requires_grad=True)\n"
                    ]
                }
            ],
            "source": [
                "for param in model.parameters():\n",
                "    print(param)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "from collections import OrderedDict\n",
                "valid_keys = glob.glob('../../../datasets/pocket_sample_70w/test/*')\n",
                "# valid_keys = [v.split('/')[-1] for v in valid_keys]\n",
                "\n",
                "# dude_gene =  list(OrderedDict.fromkeys([v.split('\\\\')[1].split('_')[0] for v in valid_keys]))\n",
                "# # ../../../datasets\\pocket_sample_70w\\test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'../../../datasets/pocket_sample_70w/test\\\\ALDH1_4x4l_active_V_ligand_0'"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "valid_keys[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'1.8.1+cpu'"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import torch\n",
                "torch.cuda.is_available()\n",
                "torch.__version__"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "sample = torch.tensor(np.random.randn(10,8,10,10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([10, 8, 10, 10])"
                        ]
                    },
                    "execution_count": 44,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sample.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "NoneType"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "type(None)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "ename": "AttributeError",
                    "evalue": "'Tensor' object has no attribute 'sum_'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-45-4f2841ef63e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sum_'"
                    ]
                }
            ],
            "source": [
                "sample.sum(dim = 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ImportError",
                    "evalue": "Building module algos failed: ['distutils.errors.DistutilsPlatformError: Unable to find vcvarsall.bat\\n']",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mDistutilsPlatformError\u001b[0m                    Traceback (most recent call last)",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\site-packages\\pyximport\\pyximport.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, pyxfilename, pyxbuild_dir, is_package, build_inplace, language_level, so_path)\u001b[0m\n\u001b[0;32m    214\u001b[0m             so_path = build_module(module_name, pyxfilename, pyxbuild_dir,\n\u001b[1;32m--> 215\u001b[1;33m                                    inplace=build_inplace, language_level=language_level)\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mso_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\site-packages\\pyximport\\pyximport.py\u001b[0m in \u001b[0;36mbuild_module\u001b[1;34m(name, pyxfilename, pyxbuild_dir, inplace, language_level)\u001b[0m\n\u001b[0;32m    190\u001b[0m                                   \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m                                   reload_support=pyxargs.reload_support)\n\u001b[0m\u001b[0;32m    192\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mso_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot find: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mso_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\site-packages\\pyximport\\pyxbuild.py\u001b[0m in \u001b[0;36mpyx_to_dll\u001b[1;34m(filename, ext, force_rebuild, build_in_temp, pyxbuild_dir, setup_args, reload_support, inplace)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mobj_build_ext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_command_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"build_ext\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_commands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[0mso_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj_build_ext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\dist.py\u001b[0m in \u001b[0;36mrun_commands\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\dist.py\u001b[0m in \u001b[0;36mrun_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0mcmd_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_finalized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m         \u001b[0mcmd_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhave_run\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\site-packages\\Cython\\Distutils\\old_build_ext.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[0m_build_ext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_ext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;31m# Now actually compile and link everything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\site-packages\\Cython\\Distutils\\old_build_ext.py\u001b[0m in \u001b[0;36mbuild_extensions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;31m# Call original build_extensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0m_build_ext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_ext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mbuild_extensions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_extensions_serial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36m_build_extensions_serial\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter_build_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mbuild_extension\u001b[1;34m(self, ext)\u001b[0m\n\u001b[0;32m    533\u001b[0m                                          \u001b[0mextra_postargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m                                          depends=ext.depends)\n\u001b[0m\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\_msvccompiler.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, sources, output_dir, macros, include_dirs, debug, extra_preargs, extra_postargs, depends)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         compile_info = self._setup_compile(output_dir, macros, include_dirs,\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\_msvccompiler.py\u001b[0m in \u001b[0;36minitialize\u001b[1;34m(self, plat_name)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mvc_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_vc_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvc_env\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\_msvccompiler.py\u001b[0m in \u001b[0;36m_get_vc_env\u001b[1;34m(plat_spec)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvcvarsall\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mDistutilsPlatformError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unable to find vcvarsall.bat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mDistutilsPlatformError\u001b[0m: Unable to find vcvarsall.bat",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-11-5b5b5008daf7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgraphformer_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgetEdge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrdkit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAllChem\u001b[0m  \u001b[1;32mas\u001b[0m \u001b[0mAllChem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_keys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32md:\\GNN_FOR_PROTEIN\\codes\\GNN_DTI-master\\GNN_graphformer\\graphformer_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyximport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mpyximport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'include_dirs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_include\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0malgos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\site-packages\\pyximport\\pyximport.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(self, fullname)\u001b[0m\n\u001b[0;32m    460\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyxbuild_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m                                  \u001b[0mbuild_inplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m                                  language_level=self.language_level)\n\u001b[0m\u001b[0;32m    463\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\site-packages\\pyximport\\pyximport.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, pyxfilename, pyxbuild_dir, is_package, build_inplace, language_level, so_path)\u001b[0m\n\u001b[0;32m    229\u001b[0m                 name, traceback.format_exception_only(*sys.exc_info()[:2])))\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"raise exc, None, tb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'exc'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tb'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\site-packages\\pyximport\\pyximport.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, pyxfilename, pyxbuild_dir, is_package, build_inplace, language_level, so_path)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mmodule_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             so_path = build_module(module_name, pyxfilename, pyxbuild_dir,\n\u001b[1;32m--> 215\u001b[1;33m                                    inplace=build_inplace, language_level=language_level)\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mso_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_package\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__path__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\site-packages\\pyximport\\pyximport.py\u001b[0m in \u001b[0;36mbuild_module\u001b[1;34m(name, pyxfilename, pyxbuild_dir, inplace, language_level)\u001b[0m\n\u001b[0;32m    189\u001b[0m                                   \u001b[0msetup_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                                   \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m                                   reload_support=pyxargs.reload_support)\n\u001b[0m\u001b[0;32m    192\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mso_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot find: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mso_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\site-packages\\pyximport\\pyxbuild.py\u001b[0m in \u001b[0;36mpyx_to_dll\u001b[1;34m(filename, ext, force_rebuild, build_in_temp, pyxbuild_dir, setup_args, reload_support, inplace)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mobj_build_ext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_command_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"build_ext\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_commands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[0mso_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj_build_ext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj_build_ext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\dist.py\u001b[0m in \u001b[0;36mrun_commands\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    964\u001b[0m         \"\"\"\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[1;31m# -- Methods that operate on its Commands --------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\dist.py\u001b[0m in \u001b[0;36mrun_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[0mcmd_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_command_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0mcmd_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_finalized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m         \u001b[0mcmd_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhave_run\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\site-packages\\Cython\\Distutils\\old_build_ext.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0moptimization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable_optimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[0m_build_ext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_ext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;31m# Now actually compile and link everything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_extensions_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\site-packages\\Cython\\Distutils\\old_build_ext.py\u001b[0m in \u001b[0;36mbuild_extensions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcython_sources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;31m# Call original build_extensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0m_build_ext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_ext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcython_sources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mbuild_extensions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_extensions_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_extensions_serial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_build_extensions_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36m_build_extensions_serial\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextensions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter_build_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mbuild_extension\u001b[1;34m(self, ext)\u001b[0m\n\u001b[0;32m    532\u001b[0m                                          \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                                          \u001b[0mextra_postargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m                                          depends=ext.depends)\n\u001b[0m\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[1;31m# XXX outdated variable, kept here in case third-part code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\_msvccompiler.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, sources, output_dir, macros, include_dirs, debug, extra_preargs, extra_postargs, depends)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         compile_info = self._setup_compile(output_dir, macros, include_dirs,\n\u001b[0;32m    320\u001b[0m                                            sources, depends, extra_postargs)\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\_msvccompiler.py\u001b[0m in \u001b[0;36minitialize\u001b[1;34m(self, plat_name)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mplat_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPLAT_TO_VCVARS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplat_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mvc_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_vc_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvc_env\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             raise DistutilsPlatformError(\"Unable to find a compatible \"\n",
                        "\u001b[1;32m~\\Anaconda3\\envs\\DGL\\lib\\distutils\\_msvccompiler.py\u001b[0m in \u001b[0;36m_get_vc_env\u001b[1;34m(plat_spec)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0mvcvarsall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_find_vcvarsall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvcvarsall\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mDistutilsPlatformError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unable to find vcvarsall.bat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mImportError\u001b[0m: Building module algos failed: ['distutils.errors.DistutilsPlatformError: Unable to find vcvarsall.bat\\n']"
                    ]
                }
            ],
            "source": [
                "from graphformer_utils import getEdge\n",
                "   \n",
                "import rdkit.AllChem  as AllChem\n",
                "with open(valid_keys[0], 'rb') as f:\n",
                "    \n",
                "    m1,_,m2,_ = pickle.load(f)\n",
                "edge_index, edge_attr= getEdge(m1)\n",
                "    \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "with open('./keys/test_keys.pkl','rb') as f:\n",
                "    keys = pickle.load(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "37879"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(keys)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_dir = '../../data/pocket_data_fix/pocket_data'\n",
                "with open(data_dir+'/'+keys[0], 'rb') as f:\n",
                "    \n",
                "    m1,m2 = pickle.load(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<rdkit.Chem.rdchem.Mol at 0x1498db8380f0>"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "m1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<rdkit.Chem.rdchem.Mol at 0x14983e8967b0>"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "m2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "a = list(np.arange(100))\n",
                "np.random.shuffle(a)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "1.3888888888888888"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "5000"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "a,b,c,d = [1,1,0,0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;31mSignature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mSource:\u001b[0m   \n",
                        "\u001b[0;32mdef\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"einsum(equation, *operands) -> Tensor\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m    Sums the product of the elements of the input :attr:`operands` along dimensions specified using a notation\u001b[0m\n",
                        "\u001b[0;34m    based on the Einstein summation convention.\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m    Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\u001b[0m\n",
                        "\u001b[0;34m    in a short-hand format based on the Einstein summation convention, given by :attr:`equation`. The details of\u001b[0m\n",
                        "\u001b[0;34m    this format are described below, but the general idea is to label every dimension of the input :attr:`operands`\u001b[0m\n",
                        "\u001b[0;34m    with some subscript and define which subscripts are part of the output. The output is then computed by summing\u001b[0m\n",
                        "\u001b[0;34m    the product of the elements of the :attr:`operands` along the dimensions whose subscripts are not part of the\u001b[0m\n",
                        "\u001b[0;34m    output. For example, matrix multiplication can be computed using einsum as `torch.einsum(\"ij,jk->ik\", A, B)`.\u001b[0m\n",
                        "\u001b[0;34m    Here, j is the summation subscript and i and k the output subscripts (see section below for more details on why).\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m    Equation:\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m        The :attr:`equation` string specifies the subscripts (lower case letters `['a', 'z']`) for each dimension of\u001b[0m\n",
                        "\u001b[0;34m        the input :attr:`operands` in the same order as the dimensions, separating subcripts for each operand by a\u001b[0m\n",
                        "\u001b[0;34m        comma (','), e.g. `'ij,jk'` specify subscripts for two 2D operands. The dimensions labeled with the same subscript\u001b[0m\n",
                        "\u001b[0;34m        must be broadcastable, that is, their size must either match or be `1`. The exception is if a subscript is\u001b[0m\n",
                        "\u001b[0;34m        repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\u001b[0m\n",
                        "\u001b[0;34m        must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\u001b[0m\n",
                        "\u001b[0;34m        appear exactly once in the :attr:`equation` will be part of the output, sorted in increasing alphabetical order.\u001b[0m\n",
                        "\u001b[0;34m        The output is computed by multiplying the input :attr:`operands` element-wise, with their dimensions aligned based\u001b[0m\n",
                        "\u001b[0;34m        on the subscripts, and then summing out the dimensions whose subscripts are not part of the output.\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m        Optionally, the output subscripts can be explicitly defined by adding an arrow ('->') at the end of the equation\u001b[0m\n",
                        "\u001b[0;34m        followed by the subscripts for the output. For instance, the following equation computes the transpose of a\u001b[0m\n",
                        "\u001b[0;34m        matrix multiplication: 'ij,jk->ki'. The output subscripts must appear at least once for some input operand and\u001b[0m\n",
                        "\u001b[0;34m        at most once for the output.\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m        Ellipsis ('...') can be used in place of subscripts to broadcast the dimensions covered by the ellipsis.\u001b[0m\n",
                        "\u001b[0;34m        Each input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts,\u001b[0m\n",
                        "\u001b[0;34m        e.g. for an input operand with 5 dimensions, the ellipsis in the equation `'ab...c'` cover the third and fourth\u001b[0m\n",
                        "\u001b[0;34m        dimensions. The ellipsis does not need to cover the same number of dimensions across the :attr:`operands` but the\u001b[0m\n",
                        "\u001b[0;34m        'shape' of the ellipsis (the size of the dimensions covered by them) must broadcast together. If the output is not\u001b[0m\n",
                        "\u001b[0;34m        explicitly defined with the arrow ('->') notation, the ellipsis will come first in the output (left-most dimensions),\u001b[0m\n",
                        "\u001b[0;34m        before the subscript labels that appear exactly once for the input operands. e.g. the following equation implements\u001b[0m\n",
                        "\u001b[0;34m        batch matrix multiplication `'...ij,...jk'`.\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m        A few final notes: the equation may contain whitespaces between the different elements (subscripts, ellipsis,\u001b[0m\n",
                        "\u001b[0;34m        arrow and comma) but something like `'. . .'` is not valid. An empty string `''` is valid for scalar operands.\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m    .. note::\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m        ``torch.einsum`` handles ellipsis ('...') differently from NumPy in that it allows dimensions\u001b[0m\n",
                        "\u001b[0;34m        covered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output.\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m    .. note::\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m        This function does not optimize the given expression, so a different formula for the same computation may\u001b[0m\n",
                        "\u001b[0;34m        run faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\u001b[0m\n",
                        "\u001b[0;34m        can optimize the formula for you.\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m    Args:\u001b[0m\n",
                        "\u001b[0;34m        equation (string): The subscripts for the Einstein summation.\u001b[0m\n",
                        "\u001b[0;34m        operands (Tensor): The operands to compute the Einstein sum of.\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m    Examples::\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m        # trace\u001b[0m\n",
                        "\u001b[0;34m        >>> torch.einsum('ii', torch.randn(4, 4))\u001b[0m\n",
                        "\u001b[0;34m        tensor(-1.2104)\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m        # diagonal\u001b[0m\n",
                        "\u001b[0;34m        >>> torch.einsum('ii->i', torch.randn(4, 4))\u001b[0m\n",
                        "\u001b[0;34m        tensor([-0.1034,  0.7952, -0.2433,  0.4545])\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m        # outer product\u001b[0m\n",
                        "\u001b[0;34m        >>> x = torch.randn(5)\u001b[0m\n",
                        "\u001b[0;34m        >>> y = torch.randn(4)\u001b[0m\n",
                        "\u001b[0;34m        >>> torch.einsum('i,j->ij', x, y)\u001b[0m\n",
                        "\u001b[0;34m        tensor([[ 0.1156, -0.2897, -0.3918,  0.4963],\u001b[0m\n",
                        "\u001b[0;34m                [-0.3744,  0.9381,  1.2685, -1.6070],\u001b[0m\n",
                        "\u001b[0;34m                [ 0.7208, -1.8058, -2.4419,  3.0936],\u001b[0m\n",
                        "\u001b[0;34m                [ 0.1713, -0.4291, -0.5802,  0.7350],\u001b[0m\n",
                        "\u001b[0;34m                [ 0.5704, -1.4290, -1.9323,  2.4480]])\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m        # batch matrix multiplication\u001b[0m\n",
                        "\u001b[0;34m        >>> As = torch.randn(3,2,5)\u001b[0m\n",
                        "\u001b[0;34m        >>> Bs = torch.randn(3,5,4)\u001b[0m\n",
                        "\u001b[0;34m        >>> torch.einsum('bij,bjk->bik', As, Bs)\u001b[0m\n",
                        "\u001b[0;34m        tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\u001b[0m\n",
                        "\u001b[0;34m                [-1.6706, -0.8097, -0.8025, -2.1183]],\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m                [[ 4.2239,  0.3107, -0.5756, -0.2354],\u001b[0m\n",
                        "\u001b[0;34m                [-1.4558, -0.3460,  1.5087, -0.8530]],\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m                [[ 2.8153,  1.8787, -4.3839, -1.2112],\u001b[0m\n",
                        "\u001b[0;34m                [ 0.3728, -2.1131,  0.0921,  0.8305]]])\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m        # batch permute\u001b[0m\n",
                        "\u001b[0;34m        >>> A = torch.randn(2, 3, 4, 5)\u001b[0m\n",
                        "\u001b[0;34m        >>> torch.einsum('...ij->...ji', A).shape\u001b[0m\n",
                        "\u001b[0;34m        torch.Size([2, 3, 5, 4])\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m        # equivalent to torch.nn.functional.bilinear\u001b[0m\n",
                        "\u001b[0;34m        >>> A = torch.randn(3,5,4)\u001b[0m\n",
                        "\u001b[0;34m        >>> l = torch.randn(2,5)\u001b[0m\n",
                        "\u001b[0;34m        >>> r = torch.randn(2,4)\u001b[0m\n",
                        "\u001b[0;34m        >>> torch.einsum('bn,anm,bm->ba', l, A, r)\u001b[0m\n",
                        "\u001b[0;34m        tensor([[-0.3430, -5.2405,  0.4494],\u001b[0m\n",
                        "\u001b[0;34m                [ 0.3311,  5.5201, -3.0356]])\u001b[0m\n",
                        "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m        \u001b[0;31m# the old interface of passing the operands as one list argument\u001b[0m\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m        \u001b[0m_operands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m        \u001b[0;31m# recurse incase operands contains value that has torch function\u001b[0m\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m        \u001b[0;31m# in the original implementation this line is omitted\u001b[0m\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
                        "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/rdkit_good/lib/python3.7/site-packages/torch/functional.py\n",
                        "\u001b[0;31mType:\u001b[0m      function\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "torch.einsum??"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "4"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len([None,1,0,None])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "with open('/home/chengeng/project/codes/GNN_graphformer/keys/test.txt','r')as f:\n",
                "    test_dude_gene = f.readlines()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[0;31mDocstring:\u001b[0m\n",
                        "next(iterator[, default])\n",
                        "\n",
                        "Return the next item from the iterator. If default is given and the iterator\n",
                        "is exhausted, it is returned instead of raising StopIteration.\n",
                        "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
                    ]
                }
            ],
            "source": [
                "next??"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import DataLoader                                     \n",
                "from graphformer_dataset import graphformerDataset, collate_fn, DTISampler"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_weights = [0.1,0.2,0.2,0.3,0.2]\n",
                "train_sampler = DTISampler(train_weights, len(train_weights), replacement=True)                     \n",
                "train_dataloader = DataLoader([1,2,3,4,5], 5, \\\n",
                "    shuffle=False,\\\n",
                "    sampler = train_sampler)#动态采样"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([1, 4, 2, 3, 3])\n",
                        "tensor([4, 4, 5, 2, 4])\n",
                        "tensor([4, 2, 2, 5, 4])\n",
                        "tensor([4, 2, 5, 2, 4])\n",
                        "tensor([3, 2, 2, 2, 5])\n",
                        "tensor([4, 2, 3, 5, 5])\n",
                        "tensor([5, 2, 4, 1, 5])\n",
                        "tensor([5, 5, 5, 4, 5])\n",
                        "tensor([4, 3, 1, 1, 5])\n",
                        "tensor([5, 5, 4, 3, 3])\n"
                    ]
                }
            ],
            "source": [
                "for i in range(10):\n",
                "    for data in train_dataloader:\n",
                "        print(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = torch.tensor(np.random.randn(10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "labels =torch.tensor([0,1,1,0,0,0,0,1,1,1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "pos_num = torch.sum(labels)\n",
                "neg_num = len(labels) - pos_num\n",
                "pos_pred = y_pred[labels.bool()]\n",
                "neg_pred = y_pred[(1-labels).bool()]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([ 0.1723, -0.0048,  0.5465, -0.2397, -1.3048], dtype=torch.float64)"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "pos_pred"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor(-0.4857, dtype=torch.float64)"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.sum(neg_pred-pos_pred.reshape(-1,1))/25"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def forward(self, x, labels=None):\n",
                "    x1, attn_weights1,hidden_state1 = self.transformer(x)\n",
                "\n",
                "    logits = self.head(x1[:, 0])\n",
                "\n",
                "    if labels is not None:\n",
                "        loss_fct = CrossEntropyLoss()\n",
                "        loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
                "\n",
                "        x2, attn_weights2,hidden_state2 = self.transformer(x)\n",
                "        newlogits = self.head(x2[:, 0])\n",
                "        loss2 = loss_fct(newlogits.view(-1, self.num_classes), labels.view(-1))\n",
                "        loss+=loss2\n",
                "        \n",
                "        \n",
                "        p = torch.log_softmax(logits.view(-1, self.num_classes), dim=-1)\n",
                "        p_tec = torch.softmax(logits.view(-1, self.num_classes), dim=-1)\n",
                "        q = torch.log_softmax(newlogits.view(-1, self.num_classes), dim=-1)\n",
                "        q_tec = torch.softmax(newlogits.view(-1, self.num_classes), dim=-1)\n",
                "        kl_loss = torch.nn.functional.kl_div(p, q_tec, reduction='none').sum()\n",
                "        reverse_kl_loss = torch.nn.functional.kl_div(q, p_tec, reduction='none').sum()\n",
                "\n",
                "\n",
                "        loss += self.alpha * (kl_loss + reverse_kl_loss)\n",
                "\n",
                "        return loss\n",
                "    else:\n",
                "        return logits, attn_weights1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(model,args,optimizer,loss_fn,train_dataloader,auxiliary_loss):\n",
                "# 加入辅助函数和r_drop 方式\n",
                "            #collect losses of each iteration\n",
                "            train_losses = [] \n",
                "            train_true = []\n",
                "            train_pred = []\n",
                "            model.train()\n",
                "            if args.r_drop:\n",
                "                for i_batch, sample in enumerate(train_dataloader):\n",
                "                    model.zero_grad()\n",
                "                \n",
                "                    data_flag,data = data_to_device(sample,args.device)\n",
                "                    logits = model(args.A2_limit,data_flag,*data)\n",
                "                    newlogits = model(args.A2_limit,data_flag,*data)\n",
                "                    # logits = \n",
                "\n",
                "                    if args.loss_fn == 'bce_loss':\n",
                "                        pred  = torch.sigmoid(logits[:,1])\n",
                "                        pred = pred.view(-1)\n",
                "                        # loss = loss_fn(pred, sample.Y.to(pred.device))\n",
                "                        pred_1 = torch.sigmoid(newlogits[:,1])\n",
                "                        pred_1 = pred_1.view(-1)\n",
                "                        # loss += loss_fn(pred_1, sample.Y.to(pred.device))\n",
                "                        loss = loss_fn(pred, sample.Y.to(pred.device))\n",
                "                        loss += loss_fn(pred_1, sample.Y.to(pred.device))\n",
                "                    else:\n",
                "                        loss = loss_fn(logits, sample.Y.to(pred.device))\n",
                "                        loss += loss_fn(newlogits, sample.Y.to(pred.device))\n",
                "                \n",
                "                    # kl div\n",
                "                    p = torch.log_softmax(logits.view(-1,2), dim=-1)\n",
                "                    p_tec = torch.softmax(logits.view(-1,2), dim=-1)\n",
                "                    q = torch.log_softmax(newlogits.view(-1,2), dim=-1)\n",
                "                    q_tec = torch.softmax(newlogits.view(-1,2), dim=-1)\n",
                "                    kl_loss = torch.nn.functional.kl_div(p, q_tec, reduction='none').sum()\n",
                "                    reverse_kl_loss = torch.nn.functional.kl_div(q, p_tec, reduction='none').sum()\n",
                "                    #------------------------\n",
                "                    alpha = 5\n",
                "                    loss += alpha*(kl_loss + reverse_kl_loss)/2\n",
                "                    if args.auxiliary_loss:\n",
                "                        loss = loss  + model.deta*auxiliary_loss(pred,sample.Y.to(pred.device))\n",
                "\n",
                "\n",
                "                    loss.backward()\n",
                "                    optimizer.step()\n",
                "                    train_losses.append(loss.data.cpu().numpy())\n",
                "                    train_true.append(sample.Y.data.cpu().numpy())\n",
                "                    if pred.dim() ==2:\n",
                "                        pred = torch.softmax(pred,dim = -1)[:,1]\n",
                "                    train_pred.append(pred.data.cpu().numpy())\n",
                "            else:\n",
                "\n",
                "                for i_batch, sample in enumerate(train_dataloader):\n",
                "                    model.zero_grad()\n",
                "                \n",
                "                    data_flag,data = data_to_device(sample,args.device)\n",
                "                    pred = model(args.A2_limit,data_flag,*data)\n",
                "                    loss = loss_fn(pred, sample.Y.to(pred.device))\n",
                "                    if args.auxiliary_loss:\n",
                "                        loss += model.deta*auxiliary_loss(pred,sample.Y.to(pred.device))\n",
                "                    \n",
                "                        # loss = loss_fn(pred, sample.Y.to(pred.device))\n",
                "                    loss.backward()\n",
                "                    optimizer.step()\n",
                "                    train_losses.append(loss.data.cpu().numpy())\n",
                "                    train_true.append(sample.Y.data.cpu().numpy())\n",
                "                    if pred.dim() ==2:\n",
                "                        pred = torch.softmax(pred,dim = -1)[:,1]\n",
                "                    train_pred.append(pred.data.cpu().numpy())\n",
                "                return model,train_pred,train_losses,optimizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/caoduanhua/anaconda3/envs/pignet/lib/python3.6/site-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
                        "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "tensor(0., dtype=torch.float64)"
                        ]
                    },
                    "execution_count": 42,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "x = torch.softmax(torch.tensor(np.random.uniform(10)).reshape(-1,1),dim = -1)\n",
                "y  = torch.softmax(torch.tensor(np.random.rand(10)).reshape(-1,1),dim = -1)\n",
                "pred = torch.cat([x,1-x],dim = 1)\n",
                "target = torch.cat([y,1-y],dim = 1)\n",
                "torch.nn.functional.kl_div(pred,target,log_target=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor(-0.5000, dtype=torch.float64)"
                        ]
                    },
                    "execution_count": 38,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.nn.functional.kl_div(pred,target)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor(-1., dtype=torch.float64)"
                        ]
                    },
                    "execution_count": 39,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.nn.functional.kl_div(pred[:,0],target[:,0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[1., 0.],\n",
                            "        [1., 0.],\n",
                            "        [1., 0.],\n",
                            "        [1., 0.],\n",
                            "        [1., 0.],\n",
                            "        [1., 0.],\n",
                            "        [1., 0.],\n",
                            "        [1., 0.],\n",
                            "        [1., 0.],\n",
                            "        [1., 0.]], dtype=torch.float64)"
                        ]
                    },
                    "execution_count": 40,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "pred"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "7716fc1eb704f5e068776c01e0156f1adfe1bef04458461b29df577577dc7b8a"
        },
        "kernelspec": {
            "display_name": "Python 3.6.12 64-bit ('pignet': conda)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.13"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
